# ============================================================================
# 0. ç¯å¢ƒé…ç½®
# ============================================================================

# 0.1 è¯»å–å½“å‰ä½ç½®ï¼Œå¹¶å°†å·¥ä½œç¯å¢ƒè®¾ç½®ä¸ºå½“å‰ä½ç½®ï¼ˆRStudio ç›¸å¯¹è·¯å¾„ï¼‰
if (requireNamespace("rstudioapi", quietly = TRUE)) {
  setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
}
cat("å½“å‰å·¥ä½œç›®å½•:", getwd(), "\n")

# 0.2 åŠ è½½åŒ…ä¸å¤šæ ¸é…ç½®
if (!require("pacman")) install.packages("pacman")
pacman::p_load(
  tidyverse,        # æ•°æ®å¤„ç†ä¸å¯è§†åŒ–
  
  lightgbm,         # LightGBM æ¨¡å‹
  recipes,          # ç‰¹å¾å·¥ç¨‹
  caret,            # ä¼ ç»Ÿå»ºæ¨¡æ¡†æ¶ï¼ˆç½‘æ ¼æœç´¢ï¼‰
  ParBayesianOptimization, # è´å¶æ–¯ä¼˜åŒ–
  shapviz,          # SHAP å¯è§†åŒ–
  DALEX,            # å¯è§£é‡Šæ€§æ¡†æ¶
  DALEXtra,         # DALEX æ‰©å±•
  parallel,         # å¹¶è¡Œè®¡ç®—
  doParallel,       # å¹¶è¡Œåç«¯
  foreach,          # å¹¶è¡Œå¾ªç¯
  tidymodels,       # ç°ä»£å»ºæ¨¡æ¡†æ¶
  Matrix,           # ç¨€ç–çŸ©é˜µ
  ggthemes,         # ggplot2 ä¸»é¢˜æ‰©å±•
  patchwork         # å›¾å½¢æ‹¼æ¥
)

# è·å–é€»è¾‘æ ¸å¿ƒæ•°ï¼Œé¢„ç•™2ä¸ªæ ¸å¿ƒç»™ç³»ç»Ÿ
n_cores <- max(1, parallel::detectCores() - 2)
cat("ä½¿ç”¨æ ¸å¿ƒæ•°:", n_cores, "\n")

# 0.3 è‡ªåŠ¨åˆ›å»ºæ–‡ä»¶å¤¹ç»“æ„
dirs <- c("0Data", "1Models", "1.5Tables", "2Figs", "3Permutation", "4SHAP")
sapply(dirs, function(d) if (!dir.exists(d)) dir.create(d, recursive = TRUE))
cat("æ–‡ä»¶å¤¹ç»“æ„å·²å°±ç»ª\n")

# è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿å¯é‡å¤æ€§
set.seed(42)

# ============================================================================
# 1. æ•°æ®è¯»å–ä¸é¢„å¤„ç†
# ============================================================================

cat("\n========== 1. æ•°æ®è¯»å–ä¸é¢„å¤„ç† ==========\n")

# 1.1 åŠ è½½ diamonds æ•°æ®é›†ï¼ˆéšæœºæŠ½æ · 50%ï¼‰
data("diamonds", package = "ggplot2")
df_raw <- as_tibble(diamonds) %>%
  sample_frac(0.3)  # ä¿ç•™ 30% çš„æ ·æœ¬ï¼Œä½¿ç”¨æ–‡ä»¶é¡¶éƒ¨çš„ set.seed(42) ä¿è¯å¯é‡å¤æ€§

cat("å·²ä»åŸå§‹ diamonds ä¸­éšæœºæŠ½å– 50% æ ·æœ¬ã€‚\n")
cat("æŠ½æ ·åæ•°æ®ç»´åº¦:", nrow(df_raw), "è¡Œ x", ncol(df_raw), "åˆ—\n")
cat("å˜é‡å:", paste(names(df_raw), collapse = ", "), "\n")

# 1.2 æ•°æ®æ¢ç´¢
cat("\n--- æ•°æ®ç»“æ„ ---\n")
str(df_raw)

cat("\n--- æ•°æ®æ‘˜è¦ ---\n")
summary(df_raw)

# 1.3 ç¼ºå¤±å€¼æ£€æŸ¥
missing_summary <- df_raw %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(everything(), names_to = "variable", values_to = "missing_count") %>%
  mutate(missing_pct = round(missing_count / nrow(df_raw) * 100, 2))

cat("\n--- ç¼ºå¤±å€¼æ£€æŸ¥ ---\n")
print(missing_summary)

if (sum(missing_summary$missing_count) == 0) {
  cat("âœ“ æ•°æ®æ— ç¼ºå¤±å€¼\n")
} else {
  cat("âš  å­˜åœ¨ç¼ºå¤±å€¼ï¼Œå°†è¿›è¡Œå¤„ç†\n")
  df_raw <- df_raw %>% drop_na()
}

# 1.4 æ•°æ®æ¸…æ´—ï¼šç§»é™¤å¼‚å¸¸å€¼ï¼ˆx, y, z ä¸­å­˜åœ¨ 0 å€¼çš„è®°å½•ï¼‰
cat("\n--- å¼‚å¸¸å€¼æ£€æŸ¥ ---\n")
n_before <- nrow(df_raw)
df_clean <- df_raw %>%
  filter(x > 0, y > 0, z > 0)
n_after <- nrow(df_clean)
cat("ç§»é™¤ x/y/z ä¸º 0 çš„è®°å½•:", n_before - n_after, "æ¡\n")
cat("æ¸…æ´—åæ•°æ®ç»´åº¦:", nrow(df_clean), "è¡Œ x", ncol(df_clean), "åˆ—\n")

# 1.5 æœ‰åºåˆ†ç±»å˜é‡çš„æ•´æ•°ç¼–ç 
# diamonds ä¸­ cut, color, clarity æœ¬èº«å°±æ˜¯ ordered factor
# æˆ‘ä»¬å°†å…¶è½¬æ¢ä¸ºæ•´æ•°ç¼–ç ï¼Œä¿ç•™æœ‰åºä¿¡æ¯ï¼ŒLightGBM åŸç”Ÿæ”¯æŒ

cat("\n--- æœ‰åºåˆ†ç±»å˜é‡ç¼–ç  ---\n")

# æŸ¥çœ‹åŸå§‹å› å­æ°´å¹³
cat("cut æ°´å¹³:", levels(df_clean$cut), "\n")
cat("color æ°´å¹³:", levels(df_clean$color), "\n")
cat("clarity æ°´å¹³:", levels(df_clean$clarity), "\n")

# æ•´æ•°ç¼–ç ï¼šå°†æœ‰åºå› å­è½¬ä¸ºæ•´æ•°
df_encoded <- df_clean %>%
  mutate(
    cut     = as.integer(cut),      # Fair=1, Good=2, Very Good=3, Premium=4, Ideal=5
    color   = as.integer(color),    # D=1, E=2, ..., J=7
    clarity = as.integer(clarity)   # I1=1, SI2=2, ..., IF=8
  )

cat("ç¼–ç å®Œæˆï¼Œæ•°æ®ç±»å‹:\n")
cat(paste(names(df_encoded), "->", sapply(df_encoded, class), collapse = "\n"), "\n")

# ä¿å­˜æ¸…æ´—åçš„æ•°æ®
write_csv(df_encoded, "0Data/diamonds_cleaned.csv")
cat("âœ“ æ¸…æ´—åæ•°æ®å·²ä¿å­˜è‡³ 0Data/diamonds_cleaned.csv\n")

# ============================================================================
# 2. æ•°æ®åˆ†å‰²
# ============================================================================

cat("\n========== 2. æ•°æ®åˆ†å‰² (8:2) ==========\n")

# ä½¿ç”¨ tidymodels çš„ initial_split è¿›è¡Œåˆ†å±‚æŠ½æ ·ï¼ˆæŒ‰ price åˆ†å±‚ï¼‰
split_obj <- initial_split(df_encoded, prop = 0.8, strata = price)
df_train  <- training(split_obj)
df_test   <- testing(split_obj)

cat("è®­ç»ƒé›†:", nrow(df_train), "è¡Œ\n")
cat("æµ‹è¯•é›†:", nrow(df_test), "è¡Œ\n")

# åˆ†ç¦»ç‰¹å¾ä¸ç›®æ ‡å˜é‡
target_col   <- "price"
feature_cols <- setdiff(names(df_encoded), target_col)

X_train <- df_train %>% select(all_of(feature_cols))
y_train <- df_train[[target_col]]
X_test  <- df_test %>% select(all_of(feature_cols))
y_test  <- df_test[[target_col]]

# æ„å»º LightGBM æ•°æ®çŸ©é˜µ
# æŒ‡å®šåˆ†ç±»ç‰¹å¾åˆ—ï¼ˆæ•´æ•°ç¼–ç çš„æœ‰åºå˜é‡ï¼‰
cat_features <- c("cut", "color", "clarity")

dtrain <- lgb.Dataset(
  data     = as.matrix(X_train),
  label    = y_train,
  categorical_feature = cat_features
)

dtest <- lgb.Dataset(
  data      = as.matrix(X_test),
  label     = y_test,
  reference = dtrain,
  categorical_feature = cat_features
)

# ä¿å­˜è®­ç»ƒé›†å’Œæµ‹è¯•é›†
write_csv(df_train, "0Data/train_set.csv")
write_csv(df_test, "0Data/test_set.csv")
cat("âœ“ è®­ç»ƒé›†/æµ‹è¯•é›†å·²ä¿å­˜è‡³ 0Data/\n")

# ============================================================================
# 3. æ–¹æ¡ˆ A: ç½‘æ ¼æœç´¢ (Grid Search) 
# ============================================================================
# library(foreach)
# library(doParallel)
# 
# # æ³¨å†Œå¹¶è¡Œåç«¯
# cl <- makeCluster(n_cores)
# registerDoParallel(cl)
# 
# # å‚æ•°ç½‘æ ¼
# param_grid <- expand.grid(
#   learning_rate  = c(0.01, 0.05, 0.1),
#   num_leaves     = c(31, 63, 127),
#   max_depth      = c(-1, 6, 10),
#   min_data_in_leaf = c(20, 50),
#   feature_fraction = c(0.8, 1.0),
#   bagging_fraction = c(0.8, 1.0),
#   stringsAsFactors = FALSE
# )
# param_sample <- param_grid
# 
# # ============= æ–¹æ¡ˆBï¼šé¢„å…ˆæ„é€ lgb.Dataset =============
# # 1. åˆ›å»ºDatasetå¯¹è±¡
# dtrain <- lgb.Dataset(
#   data = as.matrix(X_train),
#   label = y_train,
#   categorical_feature = cat_features,
#   free_raw_data = FALSE  # å¿…é¡»è®¾ç½®ä¸ºFALSEï¼Œä¿ç•™æ•°æ®åœ¨å†…å­˜ä¸­
# )
# 
# # 2. é¢„å…ˆæ„é€ Datasetï¼ˆå…³é”®æ­¥éª¤ï¼ï¼‰
# lgb.Dataset.construct(dtrain)
# 
# # 3. å°†æ„é€ å¥½çš„Datasetåºåˆ—åŒ–å¹¶å¯¼å‡ºåˆ°worker
# #    lightgbmçš„Datasetå¯¹è±¡ä¸èƒ½ç›´æ¥ä¼ é€’ï¼Œéœ€è¦ä¿å­˜åˆ°æ–‡ä»¶å†è¯»å–
# dataset_file <- tempfile(fileext = ".bin")
# lgb.Dataset.save(dtrain, dataset_file)
# 
# # å¹¶è¡Œç½‘æ ¼æœç´¢
# grid_results <- foreach(
#   i = 1:nrow(param_sample),
#   .export = c("dataset_file", "cat_features"),  # åªå¯¼å‡ºæ–‡ä»¶è·¯å¾„
#   .packages = c("lightgbm", "dplyr"),
#   .combine = bind_rows
# ) %dopar% {
#   # 4. åœ¨æ¯ä¸ªworkerä¸­åŠ è½½é¢„å…ˆæ„é€ å¥½çš„Dataset
#   dtrain_worker <- lgb.Dataset.load(dataset_file)
#   
#   params_i <- list(
#     objective        = "regression",
#     metric           = "rmse",
#     learning_rate    = param_sample$learning_rate[i],
#     num_leaves       = param_sample$num_leaves[i],
#     max_depth        = param_sample$max_depth[i],
#     min_data_in_leaf = param_sample$min_data_in_leaf[i],
#     feature_fraction = param_sample$feature_fraction[i],
#     bagging_fraction = param_sample$bagging_fraction[i],
#     bagging_freq     = 5,
#     verbose          = -1
#   )
#   
#   cv_result <- lgb.cv(
#     params   = params_i,
#     data     = dtrain_worker,
#     nrounds  = 500,
#     nfold    = 5,
#     early_stopping_rounds = 30,
#     verbose  = -1
#   )
#   
#   tibble(
#     learning_rate    = params_i$learning_rate,
#     num_leaves       = params_i$num_leaves,
#     max_depth        = params_i$max_depth,
#     min_data_in_leaf = params_i$min_data_in_leaf,
#     feature_fraction = params_i$feature_fraction,
#     bagging_fraction = params_i$bagging_fraction,
#     best_iter        = cv_result$best_iter,
#     best_rmse        = cv_result$best_score
#   )
# }
# 
# # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
# unlink(dataset_file)
# 
# # å…³é—­å¹¶è¡Œé›†ç¾¤
# stopCluster(cl)
# registerDoSEQ()
# 
# # æŸ¥çœ‹æ•ˆæœ
# cat("å¹¶è¡Œç½‘æ ¼æœç´¢å®Œæˆï¼æœ€ä½³RMSE:", min(grid_results$best_rmse), "\n")
# # æ±‡æ€»ç»“æœ
#  grid_results_df <- bind_rows(grid_results) %>%
#   arrange(best_rmse)
# 
# cat("\n--- ç½‘æ ¼æœç´¢ Top 5 å‚æ•°ç»„åˆ ---\n")
# print(head(grid_results_df, 5))
# 
# # ä¿å­˜ç½‘æ ¼æœç´¢ç»“æœ
# write_csv(grid_results_df, "1.5Tables/grid_search_results.csv")
# cat("âœ“ ç½‘æ ¼æœç´¢ç»“æœå·²ä¿å­˜è‡³ 1.5Tables/grid_search_results.csv\n")
# 
# # æœ€ä¼˜å‚æ•°ï¼ˆç½‘æ ¼æœç´¢ï¼‰
# best_grid <- grid_results_df %>% slice(1)
# cat("\nç½‘æ ¼æœç´¢æœ€ä¼˜å‚æ•°:\n")
# print(best_grid)
# ============================================================================
# 3. æ–¹æ¡ˆ A: å¹¶è¡Œç½‘æ ¼æœç´¢ (ä¿®æ­£ç‰ˆ)
# ============================================================================
library(foreach)
library(doParallel)

# æ³¨å†Œå¹¶è¡Œåç«¯
cl <- makeCluster(n_cores)
registerDoParallel(cl)

# å‚æ•°ç½‘æ ¼
param_grid <- expand.grid(
  learning_rate  = c(0.01, 0.05, 0.1),
  num_leaves     = c(31, 63, 127),
  max_depth      = c(-1, 6, 10),
  min_data_in_leaf = c(20, 50),
  feature_fraction = c(0.8, 1.0),
  bagging_fraction = c(0.8, 1.0),
  stringsAsFactors = FALSE
)

# éšæœºæŠ½æ ·éƒ¨åˆ†å‚æ•°è¿›è¡Œæ¼”ç¤ºï¼ˆå…¨é‡è·‘å¤ªæ…¢ï¼‰
set.seed(123)
param_sample <- param_grid %>% sample_n(min(20, nrow(param_grid))) 

# --- å…³é”®ä¿®æ­£ï¼šå¯¼å‡ºåŸå§‹æ•°æ®è€Œé lgb.Dataset ---
# LightGBM Dataset æ˜¯ C++ æŒ‡é’ˆï¼Œä¸èƒ½ç›´æ¥è·¨è¿›ç¨‹ä¼ è¾“
clusterExport(cl, varlist = c("X_train", "y_train", "cat_features"), envir = environment())
clusterEvalQ(cl, {
  library(lightgbm)
  library(dplyr)
})

cat("å¼€å§‹å¹¶è¡Œç½‘æ ¼æœç´¢...\n")

grid_results <- foreach(
  i = 1:nrow(param_sample),
  .combine = bind_rows,
  .packages = c("lightgbm", "dplyr"),
  .errorhandling = "pass" # é˜²æ­¢å•ä¸ªä»»åŠ¡æŠ¥é”™å¯¼è‡´æ•´ä½“å´©æºƒ
) %dopar% {
  
  # 1. åœ¨ Worker å†…éƒ¨æ„å»º Dataset (è¿™æ˜¯å¿…é¡»çš„)
  # è™½ç„¶æœ‰é‡å¤æ„å»ºçš„å¼€é”€ï¼Œä½†ä¿è¯äº†å†…å­˜å®‰å…¨
  dtrain_worker <- lgb.Dataset(
    data = as.matrix(X_train),
    label = y_train,
    categorical_feature = cat_features,
    free_raw_data = FALSE,
    params = list(verbose = -1)
  )
  
  # 2. è®¾å®šå‚æ•°
  params_i <- list(
    objective        = "regression",
    metric           = "rmse",
    learning_rate    = param_sample$learning_rate[i],
    num_leaves       = param_sample$num_leaves[i],
    max_depth        = param_sample$max_depth[i],
    min_data_in_leaf = param_sample$min_data_in_leaf[i],
    feature_fraction = param_sample$feature_fraction[i],
    bagging_fraction = param_sample$bagging_fraction[i],
    bagging_freq     = 5,
    verbose          = -1,
    num_threads      = 1 # å…³é”®ï¼šWorkerå†…éƒ¨å¼ºåˆ¶å•çº¿ç¨‹ï¼Œé˜²æ­¢CPUè¿‡è½½
  )
  
  # 3. äº¤å‰éªŒè¯
  cv_result <- tryCatch({
    lgb.cv(
      params   = params_i,
      data     = dtrain_worker,
      nrounds  = 1000, 
      nfold    = 3,     # æ¼”ç¤ºç”¨3æŠ˜ï¼Œå®é™…å»ºè®®5æŠ˜
      early_stopping_rounds = 30,
      verbose  = -1,
      stratified = FALSE # å›å½’ä»»åŠ¡é€šå¸¸è®¾ä¸º FALSE
    )
  }, error = function(e) return(NULL))
  
  if(is.null(cv_result)) return(NULL)
  
  # 4. è¿”å›ç»“æœ
  tibble(
    learning_rate    = params_i$learning_rate,
    num_leaves       = params_i$num_leaves,
    max_depth        = params_i$max_depth,
    best_iter        = cv_result$best_iter,
    best_rmse        = cv_result$best_score
  )
}

stopCluster(cl)
registerDoSEQ()

# å¤„ç†ç»“æœ
grid_results_df <- grid_results %>% arrange(best_rmse)
cat("ç½‘æ ¼æœç´¢æœ€ä¼˜ RMSE:", min(grid_results_df$best_rmse), "\n")

# ============= å¯é€‰çš„ï¼šä¿å­˜ç»“æœ =============
# saveRDS(grid_results, "grid_search_results.rds")
# write.csv(grid_results, "grid_search_results.csv", row.names = FALSE)
# ============================================================================
# 4. æ–¹æ¡ˆ B: è´å¶æ–¯ä¼˜åŒ– (Bayesian Optimization)
# ============================================================================
# ============================================================================
# 4. è´å¶æ–¯ä¼˜åŒ–ï¼ˆå®Œå…¨è‡ªåŒ…å«çš„workerèŠ‚ç‚¹ï¼‰
# ============================================================================

lgb_bayesian_objective <- function(
    learning_rate, num_leaves, max_depth, min_data_in_leaf,
    feature_fraction, bagging_fraction, lambda_l1, lambda_l2
) {
  
  # 1. åŠ è½½åŒ…
  library(lightgbm)
  
  # 2. â­â­â­ åœ¨workerèŠ‚ç‚¹é‡æ–°åˆ›å»ºæ•°æ®ï¼
  # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦è®¿é—®X_train, y_train, cat_features
  # è¿™äº›å¯¹è±¡å¿…é¡»ä»ä¸»èŠ‚ç‚¹å¯¼å‡º
  
  dtrain_worker <- lgb.Dataset(
    data = as.matrix(X_train),
    label = y_train,
    categorical_feature = cat_features
  )
  
  # 3. è®¾ç½®å‚æ•°
  params <- list(
    objective        = "regression",
    metric           = "rmse",
    learning_rate    = learning_rate,
    num_leaves       = as.integer(num_leaves),
    max_depth        = as.integer(max_depth),
    min_data_in_leaf = as.integer(min_data_in_leaf),
    feature_fraction = feature_fraction,
    bagging_fraction = bagging_fraction,
    lambda_l1        = lambda_l1,
    lambda_l2        = lambda_l2,
    bagging_freq     = 5,
    verbose          = -1,
    num_threads      = 1
  )
  
  # 4. æ‰§è¡ŒCV
  cv_result <- lgb.cv(
    params   = params,
    data     = dtrain_worker,  # ä½¿ç”¨workerè‡ªå·±çš„æ•°æ®
    nrounds  = 1000,
    nfold    = 5,
    early_stopping_rounds = 50,
    verbose  = -1
  )
  
  list(Score = -cv_result$best_score, Pred = 0)
}

# ============================================================================
# å¹¶è¡Œé…ç½®
# ============================================================================

library(parallel)
library(doParallel)
library(lightgbm)

# è®¾ç½®å¹¶è¡Œé›†ç¾¤
n_cores <- max(1, detectCores() - 2)
cl <- makeCluster(n_cores)

# â­â­â­ å…³é”®æ­¥éª¤1ï¼šåœ¨workerèŠ‚ç‚¹åŠ è½½åŒ…
clusterEvalQ(cl, {
  library(lightgbm)
  NULL
})
search_bounds <- list(
  learning_rate = c(0.01, 0.3),
  num_leaves = c(20, 100),
  max_depth = c(3, 10),
  min_data_in_leaf = c(5, 50),      # æ·»åŠ è¿™ä¸ªå‚æ•°ï¼
  feature_fraction = c(0.5, 1.0),    # æ·»åŠ è¿™ä¸ªå‚æ•°ï¼
  bagging_fraction = c(0.5, 1.0),    # æ·»åŠ è¿™ä¸ªå‚æ•°ï¼
  lambda_l1 = c(0, 1),              # æ·»åŠ è¿™ä¸ªå‚æ•°ï¼
  lambda_l2 = c(0, 1)               # æ·»åŠ è¿™ä¸ªå‚æ•°ï¼
)
# â­â­â­ å…³é”®æ­¥éª¤2ï¼šæŠŠæ•°æ®å¯¼å‡ºåˆ°workerèŠ‚ç‚¹
clusterExport(cl, 
              c("X_train", "y_train", "cat_features" , "search_bounds"),  # æ³¨æ„ï¼šæ˜¯dtrainçš„åŸææ–™ï¼Œä¸æ˜¯dtrainæœ¬èº«ï¼
              envir = environment())

registerDoParallel(cl)

# æ‰§è¡Œè´å¶æ–¯ä¼˜åŒ–
set.seed(42)
bayes_result <- bayesOpt(
  FUN       = lgb_bayesian_objective,
  bounds    = search_bounds,
  initPoints = 10,
  iters.n   = 20,
  iters.k   = 5,
  parallel  = TRUE,
  acq       = "ucb",
  kappa     = 2.576,
  verbose   = 1
)

# æ¸…ç†
stopCluster(cl)
registerDoSEQ()
cat("\n--- è´å¶æ–¯ä¼˜åŒ–æœ€ä¼˜å‚æ•° ---\n")
best_bayes_params <- getBestPars(bayes_result)
print(best_bayes_params)

# ä¿å­˜è´å¶æ–¯ä¼˜åŒ–å†å²
bayes_history <- bayes_result$scoreSummary %>%
  as_tibble()
write_csv(bayes_history, "1.5Tables/bayesian_optimization_history.csv")
cat("âœ“ è´å¶æ–¯ä¼˜åŒ–å†å²å·²ä¿å­˜è‡³ 1.5Tables/bayesian_optimization_history.csv\n")


# ============================================================================
# 6. ä½¿ç”¨æœ€ä¼˜å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
# ============================================================================


cat("\n========== 6. è®­ç»ƒæœ€ç»ˆæ¨¡å‹ ==========\n")

# 6.1 ä»ç½‘æ ¼æœç´¢å’Œè´å¶æ–¯ä¼˜åŒ–ä¸­å„é€‰å‡ºæœ€ä½³æ¨¡å‹
cat("ä»ä¸¤ç§ä¼˜åŒ–æ–¹æ³•ä¸­ç­›é€‰å† å†›æ¨¡å‹...\n")

# ç½‘æ ¼æœç´¢çš„æœ€ä½³å‚æ•°
best_grid_params <- grid_results %>% slice(1)
grid_rmse <- best_grid_params$best_rmse

# è´å¶æ–¯ä¼˜åŒ–çš„æœ€ä½³å‚æ•°
bayes_rmse <- bayes_result$scoreSummary %>%
  as_tibble() %>%
  filter(Score == max(Score)) %>%
  slice(1) %>%
  pull(Score) %>%
  abs()  # è´å¶æ–¯ä¼˜åŒ–å­˜å‚¨çš„æ˜¯è´Ÿå€¼

cat("\nç½‘æ ¼æœç´¢æœ€ä½³ RMSE:", grid_rmse, "\n")
cat("è´å¶æ–¯ä¼˜åŒ–æœ€ä½³ RMSE:", bayes_rmse, "\n")

# é€‰æ‹©ä¸¤è€…ä¸­æ›´ä¼˜çš„ï¼ˆRMSE æ›´å°ï¼‰
if (grid_rmse <= bayes_rmse) {
  cat("\nğŸ† å† å†›æ¨¡å‹: ç½‘æ ¼æœç´¢\n")
  final_params <- list(
    objective        = "regression",
    metric           = "rmse",
    learning_rate    = best_grid_params$learning_rate,
    num_leaves       = as.integer(best_grid_params$num_leaves),
    max_depth        = as.integer(best_grid_params$max_depth),
    min_data_in_leaf = as.integer(best_grid_params$min_data_in_leaf),
    feature_fraction = best_grid_params$feature_fraction,
    bagging_fraction = best_grid_params$bagging_fraction,
    bagging_freq     = 5,
    verbose          = -1
  )
  champion_source <- "Grid Search"
} else {
  cat("\nğŸ† å† å†›æ¨¡å‹: è´å¶æ–¯ä¼˜åŒ–\n")
  final_params <- list(
    objective        = "regression",
    metric           = "rmse",
    learning_rate    = best_bayes_params$learning_rate,
    num_leaves       = as.integer(best_bayes_params$num_leaves),
    max_depth        = as.integer(best_bayes_params$max_depth),
    min_data_in_leaf = as.integer(best_bayes_params$min_data_in_leaf),
    feature_fraction = best_bayes_params$feature_fraction,
    bagging_fraction = best_bayes_params$bagging_fraction,
    bagging_freq     = 5,
    lambda_l1        = best_bayes_params$lambda_l1,
    lambda_l2        = best_bayes_params$lambda_l2,
    verbose          = -1
  )
  champion_source <- "Bayesian Optimization"
}

cat("\nå† å†›æ¨¡å‹å‚æ•°:\n")
print(final_params)


cat("æœ€ç»ˆå‚æ•°:\n")
print(final_params)

# 6.1 äº¤å‰éªŒè¯ç¡®å®šæœ€ä¼˜è¿­ä»£è½®æ•°
cat("\nç¡®å®šæœ€ä¼˜è¿­ä»£è½®æ•°...\n")
cv_final <- lgb.cv(
  params   = final_params,
  data     = dtrain,
  nrounds  = 2000,
  nfold    = 5,
  early_stopping_rounds = 100,
  verbose  = -1
)

best_nrounds <- cv_final$best_iter
cat("æœ€ä¼˜è¿­ä»£è½®æ•°:", best_nrounds, "\n")
cat("CV æœ€ä¼˜ RMSE:", cv_final$best_score, "\n")

# 6.2 ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®è®­ç»ƒæœ€ç»ˆæ¨¡å‹
final_model <- lgb.train(
  params  = final_params,
  data    = dtrain,
  nrounds = best_nrounds,
  verbose = -1
)

# 6.3 ä¿å­˜æ¨¡å‹
lgb.save(final_model, "1Models/lightgbm_final_model.txt")
cat("âœ“ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³ 1Models/lightgbm_final_model.txt\n")

# ============================================================================
# 7. æ¨¡å‹é¢„æµ‹ä¸è¯„ä¼°
# ============================================================================

cat("\n========== 7. æ¨¡å‹é¢„æµ‹ä¸è¯„ä¼° ==========\n")

# 7.1 åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹
y_pred <- predict(final_model, as.matrix(X_test))

# 7.2 è®¡ç®—è¯„ä¼°æŒ‡æ ‡
calc_metrics <- function(actual, predicted) {
  rmse_val <- sqrt(mean((actual - predicted)^2))
  mae_val  <- mean(abs(actual - predicted))
  ss_res   <- sum((actual - predicted)^2)
  ss_tot   <- sum((actual - mean(actual))^2)
  r2_val   <- 1 - ss_res / ss_tot
  mape_val <- mean(abs((actual - predicted) / actual)) * 100
  
  tibble(
    Metric = c("RMSE", "MAE", "R_squared", "MAPE"),
    Value  = c(rmse_val, mae_val, r2_val, mape_val)
  )
}

metrics_df <- calc_metrics(y_test, y_pred)
cat("\n--- æ¨¡å‹è¯„ä¼°æŒ‡æ ‡ ---\n")
print(metrics_df)

# 7.3 ä¿å­˜é¢„æµ‹ç»“æœ
prediction_df <- tibble(
  actual    = y_test,
  predicted = y_pred,
  residual  = y_test - y_pred
) %>%
  bind_cols(X_test)  # é™„åŠ ç‰¹å¾ä¿¡æ¯

write_csv(prediction_df, "1.5Tables/prediction_results.csv")
cat("âœ“ é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³ 1.5Tables/prediction_results.csv\n")

# 7.4 ä¿å­˜è¯„ä¼°æŒ‡æ ‡
write_csv(metrics_df, "1.5Tables/model_metrics.csv")
cat("âœ“ è¯„ä¼°æŒ‡æ ‡å·²ä¿å­˜è‡³ 1.5Tables/model_metrics.csv\n")

# 7.5 ä¿å­˜ç‰¹å¾é‡è¦æ€§
importance_df <- lgb.importance(final_model) %>%
  as_tibble()
write_csv(importance_df, "1.5Tables/feature_importance.csv")
cat("âœ“ ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜è‡³ 1.5Tables/feature_importance.csv\n")

cat("\nç‰¹å¾é‡è¦æ€§æ’å:\n")
print(importance_df)

# ============================================================================
# 8. ç»˜å›¾æ¨¡å—ï¼ˆä» CSV æ–‡ä»¶è¯»å–æ•°æ®è¿›è¡Œç»˜å›¾ï¼‰
# ============================================================================

cat("\n========== 8. ç»˜å›¾æ¨¡å— ==========\n")

# ---- ç»Ÿä¸€ä¸»é¢˜è®¾ç½® ----
my_theme <- theme_few(base_size = 12) +
  theme(
    plot.title    = element_text(face = "bold", hjust = 0.5, size = 14),
    plot.subtitle = element_text(hjust = 0.5, color = "grey40"),
    axis.title    = element_text(face = "bold"),
    legend.position = "bottom"
  )

# ---- 8.1 ä» CSV è¯»å–æ•°æ® ----
cat("ä» CSV æ–‡ä»¶è¯»å–æ•°æ®è¿›è¡Œç»˜å›¾...\n")

importance_plot_df  <- read_csv("1.5Tables/feature_importance.csv", show_col_types = FALSE)
prediction_plot_df  <- read_csv("1.5Tables/prediction_results.csv", show_col_types = FALSE)
metrics_plot_df     <- read_csv("1.5Tables/model_metrics.csv", show_col_types = FALSE)

# ---- 8.2 ç‰¹å¾é‡è¦æ€§å›¾ ----
cat("ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾...\n")

p_importance <- importance_plot_df %>%
  mutate(Feature = fct_reorder(Feature, Gain)) %>%
  ggplot(aes(x = Gain, y = Feature, fill = Gain)) +
  geom_col(show.legend = FALSE, width = 0.7) +
  scale_fill_gradient(low = "#6BAED6", high = "#08519C") +
  labs(
    title    = "LightGBM ç‰¹å¾é‡è¦æ€§ (Gain)",
    subtitle = "åŸºäºä¿¡æ¯å¢ç›Šçš„ç‰¹å¾æ’å",
    x = "ä¿¡æ¯å¢ç›Š (Gain)",
    y = "ç‰¹å¾"
  ) +
  my_theme

ggsave("2Figs/01_feature_importance.png", p_importance, 
       width = 8, height = 6, dpi = 300)
cat("âœ“ ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜è‡³ 2Figs/01_feature_importance.png\n")

# é¢å¤–ï¼šå¤šç»´åº¦ç‰¹å¾é‡è¦æ€§å›¾ï¼ˆGain + Cover + Frequencyï¼‰
p_importance_multi <- importance_plot_df %>%
  pivot_longer(cols = c(Gain, Cover, Frequency), 
               names_to = "Metric", values_to = "Value") %>%
  mutate(Feature = fct_reorder(Feature, Value, .fun = max)) %>%
  ggplot(aes(x = Value, y = Feature, fill = Metric)) +
  geom_col(position = "dodge", width = 0.7) +
  scale_fill_brewer(palette = "Set2") +
  facet_wrap(~Metric, scales = "free_x") +
  labs(
    title    = "LightGBM å¤šç»´åº¦ç‰¹å¾é‡è¦æ€§",
    subtitle = "Gain / Cover / Frequency ä¸‰ç»´åº¦å¯¹æ¯”",
    x = "é‡è¦æ€§æ•°å€¼",
    y = "ç‰¹å¾"
  ) +
  my_theme

ggsave("2Figs/02_feature_importance_multi.png", p_importance_multi, 
       width = 12, height = 6, dpi = 300)
cat("âœ“ å¤šç»´åº¦ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜è‡³ 2Figs/02_feature_importance_multi.png\n")

# ---- 8.3 é¢„æµ‹å€¼ vs çœŸå®å€¼æ•£ç‚¹å›¾ ----
cat("ç»˜åˆ¶é¢„æµ‹å€¼ vs çœŸå®å€¼æ•£ç‚¹å›¾...\n")

# è¯»å–è¯„ä¼°æŒ‡æ ‡ç”¨äºæ ‡æ³¨
rmse_val <- metrics_plot_df %>% filter(Metric == "RMSE") %>% pull(Value)
r2_val   <- metrics_plot_df %>% filter(Metric == "R_squared") %>% pull(Value)
mae_val  <- metrics_plot_df %>% filter(Metric == "MAE") %>% pull(Value)

annotation_text <- sprintf("RMSE = %.2f\nMAE = %.2f\nRÂ² = %.4f", rmse_val, mae_val, r2_val)

p_actual_vs_pred <- prediction_plot_df %>%
  ggplot(aes(x = actual, y = predicted)) +
  geom_point(alpha = 0.15, color = "#2171B5", size = 0.8) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed", linewidth = 1) +
  annotate("text", x = min(prediction_plot_df$actual) + 500, 
           y = max(prediction_plot_df$predicted) - 1000,
           label = annotation_text, hjust = 0, size = 4, fontface = "bold",
           color = "darkred") +
  labs(
    title    = "é¢„æµ‹å€¼ vs çœŸå®å€¼",
    subtitle = "çº¢è‰²è™šçº¿ä¸ºå®Œç¾é¢„æµ‹å‚è€ƒçº¿",
    x = "çœŸå®ä»·æ ¼ (Actual Price)",
    y = "é¢„æµ‹ä»·æ ¼ (Predicted Price)"
  ) +
  coord_equal() +
  my_theme

ggsave("2Figs/03_actual_vs_predicted.png", p_actual_vs_pred, 
       width = 8, height = 8, dpi = 300)
cat("âœ“ é¢„æµ‹å€¼ vs çœŸå®å€¼æ•£ç‚¹å›¾å·²ä¿å­˜è‡³ 2Figs/03_actual_vs_predicted.png\n")

# ---- 8.4 æ®‹å·®åˆ†å¸ƒå›¾ ----
cat("ç»˜åˆ¶æ®‹å·®åˆ†å¸ƒå›¾...\n")

# æ®‹å·®ç›´æ–¹å›¾
p_residual_hist <- prediction_plot_df %>%
  ggplot(aes(x = residual)) +
  geom_histogram(aes(y = after_stat(density)), bins = 60, 
                 fill = "#6BAED6", color = "white", alpha = 0.8) +
  geom_density(color = "#08519C", linewidth = 1) +
  geom_vline(xintercept = 0, color = "red", linetype = "dashed", linewidth = 0.8) +
  labs(
    title    = "æ®‹å·®åˆ†å¸ƒç›´æ–¹å›¾",
    subtitle = sprintf("æ®‹å·®å‡å€¼ = %.2f, æ ‡å‡†å·® = %.2f", 
                       mean(prediction_plot_df$residual), 
                       sd(prediction_plot_df$residual)),
    x = "æ®‹å·® (Actual - Predicted)",
    y = "å¯†åº¦"
  ) +
  my_theme

# æ®‹å·® vs é¢„æµ‹å€¼æ•£ç‚¹å›¾
p_residual_scatter <- prediction_plot_df %>%
  ggplot(aes(x = predicted, y = residual)) +
  geom_point(alpha = 0.15, color = "#2171B5", size = 0.8) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed", linewidth = 0.8) +
  geom_smooth(method = "loess", se = TRUE, color = "#E6550D", linewidth = 1) +
  labs(
    title    = "æ®‹å·® vs é¢„æµ‹å€¼",
    subtitle = "æ£€æŸ¥å¼‚æ–¹å·®æ€§å’Œç³»ç»Ÿæ€§åå·®",
    x = "é¢„æµ‹ä»·æ ¼ (Predicted Price)",
    y = "æ®‹å·® (Residual)"
  ) +
  my_theme

# åˆå¹¶æ®‹å·®å›¾
p_residual_combined <- p_residual_hist + p_residual_scatter +
  plot_annotation(
    title    = "æ®‹å·®è¯Šæ–­",
    theme    = theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5))
  )

ggsave("2Figs/04_residual_analysis.png", p_residual_combined, 
       width = 14, height = 6, dpi = 300)
cat("âœ“ æ®‹å·®åˆ†æå›¾å·²ä¿å­˜è‡³ 2Figs/04_residual_analysis.png\n")

# ---- 8.5 QQ å›¾ ----
p_qq <- prediction_plot_df %>%
  ggplot(aes(sample = residual)) +
  stat_qq(alpha = 0.3, color = "#2171B5") +
  stat_qq_line(color = "red", linewidth = 1) +
  labs(
    title = "æ®‹å·® Q-Q å›¾",
    subtitle = "æ£€éªŒæ®‹å·®æ­£æ€æ€§",
    x = "ç†è®ºåˆ†ä½æ•°",
    y = "æ ·æœ¬åˆ†ä½æ•°"
  ) +
  my_theme

ggsave("2Figs/05_residual_qq_plot.png", p_qq, 
       width = 7, height = 7, dpi = 300)
cat("âœ“ QQ å›¾å·²ä¿å­˜è‡³ 2Figs/05_residual_qq_plot.png\n")

# ============================================================================
# 9. Permutation Importance (ç½®æ¢é‡è¦æ€§åˆ†æ)
# ============================================================================

cat("\n========== 9. Permutation Importance ==========\n")
# 9.1 ä½¿ç”¨ DALEX åˆ›å»º explainer
# è‡ªå®šä¹‰é¢„æµ‹å‡½æ•°
predict_fun <- function(model, newdata) {
  predict(model, as.matrix(newdata))
}

explainer <- explain(
  model          = final_model,
  data           = as.data.frame(X_test),
  y              = y_test,
  predict_function = predict_fun,
  label          = "LightGBM",
  verbose        = FALSE
)

# 9.2 è®¡ç®—ç½®æ¢é‡è¦æ€§
cat("è®¡ç®— Permutation Importance...\n")
perm_importance <- model_parts(
  explainer,
  loss_function = loss_root_mean_square,
  B             = 500,     # ç½®æ¢æ¬¡æ•°
  type          = "difference"
)

# 9.3 ä¿å­˜ç½®æ¢é‡è¦æ€§ç»“æœ
perm_df <- perm_importance %>%
  as_tibble() %>%
  filter(variable != "_full_model_" & variable != "_baseline_") %>%
  group_by(variable) %>%
  summarise(
    mean_dropout_loss = mean(dropout_loss, na.rm = TRUE),
    sd_dropout_loss   = sd(dropout_loss, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  arrange(desc(mean_dropout_loss))

write_csv(perm_df, "3Permutation/permutation_importance.csv")
cat("âœ“ ç½®æ¢é‡è¦æ€§ç»“æœå·²ä¿å­˜è‡³ 3Permutation/permutation_importance.csv\n")
cat("\nç½®æ¢é‡è¦æ€§æ’å:\n")
print(perm_df)

# 9.4 ä¿å­˜å®Œæ•´çš„ç½®æ¢é‡è¦æ€§ï¼ˆæ¯æ¬¡ç½®æ¢çš„ç»“æœï¼‰
perm_full_df <- perm_importance %>%
  as_tibble()
write_csv(perm_full_df, "3Permutation/permutation_importance_full.csv")

# 9.5 ç»˜åˆ¶ç½®æ¢é‡è¦æ€§å›¾ï¼ˆä» CSV è¯»å–ï¼‰
cat("ç»˜åˆ¶ Permutation Importance å›¾...\n")

perm_plot_df <- read_csv("3Permutation/permutation_importance.csv", show_col_types = FALSE)

p_perm <- perm_plot_df %>%
  mutate(variable = fct_reorder(variable, mean_dropout_loss)) %>%
  ggplot(aes(x = mean_dropout_loss, y = variable)) +
  geom_col(fill = "#FB6A4A", width = 0.7) +
  geom_errorbarh(
    aes(xmin = mean_dropout_loss - sd_dropout_loss,
        xmax = mean_dropout_loss + sd_dropout_loss),
    height = 0.3, color = "grey30"
  ) +
  labs(
    title    = "Permutation Importance",
    subtitle = "ç‰¹å¾è¢«ç½®æ¢å RMSE çš„å¢åŠ é‡ï¼ˆè¶Šå¤§è¶Šé‡è¦ï¼‰",
    x = "RMSE å¢åŠ é‡ (Dropout Loss Difference)",
    y = "ç‰¹å¾"
  ) +
  my_theme

ggsave("3Permutation/06_permutation_importance.png", p_perm, 
       width = 8, height = 6, dpi = 300)
cat("âœ“ Permutation Importance å›¾å·²ä¿å­˜è‡³ 3Permutation/06_permutation_importance.png\n")

# 9.6 ä½¿ç”¨ DALEX å†…ç½®ç»˜å›¾ï¼ˆé¢å¤–å‚è€ƒï¼‰
p_perm_dalex <- plot(perm_importance) +
  labs(title = "Permutation Importance (DALEX)") +
  my_theme

ggsave("3Permutation/07_permutation_importance_dalex.png", p_perm_dalex, 
       width = 8, height = 6, dpi = 300)
cat("âœ“ DALEX Permutation å›¾å·²ä¿å­˜\n")
# ============================================================================
# 9.7 ç½®æ¢é‡è¦æ€§åˆ†å¸ƒç›´æ–¹å›¾ï¼ˆæ–°å¢ï¼‰
# ============================================================================

cat("ç»˜åˆ¶ç½®æ¢é‡è¦æ€§åˆ†å¸ƒç›´æ–¹å›¾...\n")

# ä»å®Œæ•´ç½®æ¢ç»“æœä¸­è®¡ç®—åˆ†å¸ƒ
perm_full_df <- read_csv("3Permutation/permutation_importance_full.csv", 
                          show_col_types = FALSE)

# è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
perm_stats <- perm_full_df %>%
  filter(variable != "_full_model_" & variable != "_baseline_") %>%
  group_by(variable) %>%
  summarise(
    mean_loss = mean(dropout_loss, na.rm = TRUE),
    sd_loss   = sd(dropout_loss, na.rm = TRUE),
    min_loss  = min(dropout_loss, na.rm = TRUE),
    max_loss  = max(dropout_loss, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  arrange(desc(mean_loss))

# é€‰æ‹© Top 3 ç‰¹å¾åˆ†åˆ«ç»˜åˆ¶åˆ†å¸ƒ
top_3_features <- perm_stats %>% slice(1:3) %>% pull(variable)

for (feat in top_3_features) {
  
  # æå–è¯¥ç‰¹å¾çš„æ‰€æœ‰ç½®æ¢ç»“æœ
  feat_perms <- perm_full_df %>%
    filter(variable == feat & variable != "_full_model_" & variable != "_baseline_") %>%
    pull(dropout_loss)
  
  # è®¡ç®—ç»Ÿè®¡é‡
  mean_perm <- mean(feat_perms, na.rm = TRUE)
  sd_perm   <- sd(feat_perms, na.rm = TRUE)
  
  # è®¡ç®— p-valueï¼ˆå®é™…è§‚æµ‹å€¼ç›¸å¯¹äºç½®æ¢åˆ†å¸ƒçš„æ’åï¼‰
  actual_loss <- perm_df %>% filter(variable == feat) %>% pull(mean_dropout_loss)
  p_val <- mean(feat_perms <= actual_loss, na.rm = TRUE)
  
  # ç»˜åˆ¶ç›´æ–¹å›¾
  p_perm_dist <- tibble(dropout_loss = feat_perms) %>%
    ggplot(aes(x = dropout_loss)) +
    geom_histogram(aes(y = after_stat(density)), 
                   bins = 50, fill = "#BDBDBD", color = "white", alpha = 0.8) +
    geom_density(color = "black", linewidth = 0.8) +
    # çº¢çº¿ï¼šçœŸå®è§‚å¯Ÿå€¼
    geom_vline(xintercept = actual_loss, color = "red", 
               linetype = "solid", linewidth = 1.5) +
    # æ³¨é‡Š
    annotate("text", x = Inf, y = Inf, 
             label = sprintf("Real Accuracy: %.3f\np-value: %.4f", actual_loss, p_val),
             hjust = 1.05, vjust = 1.1, 
             fontface = "bold", color = "darkred", size = 4) +
    labs(
      title    = sprintf("Permutation Importance Distribution: %s", feat),
      subtitle = "ç›´æ–¹å›¾ = 500 æ¬¡ç½®æ¢çš„åˆ†å¸ƒï¼Œçº¢çº¿ = å®é™…è§‚æµ‹å€¼",
      x = "Dropout Loss (RMSE increase)",
      y = "Density"
    ) +
    my_theme +
    theme(plot.subtitle = element_text(size = 11, color = "grey30"))
  
  # ä¿å­˜å›¾ç‰‡
  fname <- sprintf("3Permutation/08_perm_distribution_%s.png", feat)
  ggsave(fname, p_perm_dist, width = 8, height = 6, dpi = 300)
  cat(sprintf("  âœ“ %s ç½®æ¢åˆ†å¸ƒå›¾å·²ä¿å­˜\n", feat))
}

# ç»˜åˆ¶æ‰€æœ‰ç‰¹å¾çš„åˆ†å¸ƒå¯¹æ¯”ï¼ˆå°æç´å›¾ï¼‰
p_perm_violin <- perm_full_df %>%
  filter(variable != "_full_model_" & variable != "_baseline_") %>%
  mutate(variable = fct_reorder(variable, dropout_loss, .fun = median)) %>%
  ggplot(aes(x = variable, y = dropout_loss, fill = variable)) +
  geom_violin(alpha = 0.6, show.legend = FALSE) +
  geom_boxplot(width = 0.1, color = "black", fill = "white") +
  geom_point(data = perm_df, 
             aes(x = variable, y = mean_dropout_loss),
             color = "red", size = 3, shape = 4) +
  labs(
    title    = "Permutation Importance Distribution (All Features)",
    subtitle = "çº¢å‰ = å¹³å‡ç½®æ¢é‡è¦æ€§ï¼Œå°æç´ = åˆ†å¸ƒ",
    x = "ç‰¹å¾",
    y = "Dropout Loss"
  ) +
  coord_flip() +
  my_theme

ggsave("3Permutation/09_perm_distributions_violin.png", p_perm_violin, 
       width = 8, height = 6, dpi = 300)
cat("âœ“ ç½®æ¢é‡è¦æ€§å°æç´å›¾å·²ä¿å­˜\n")
# ============================================================================
# 10. SHAP åˆ†æ (SHapley Additive exPlanations)
# ============================================================================

cat("\n========== 10. SHAP åˆ†æ ==========\n")

# 10.1 è®¡ç®— SHAP å€¼
# ä½¿ç”¨ shapviz åŒ…ï¼Œç›´æ¥è°ƒç”¨ LightGBM çš„ SHAP è®¡ç®—
cat("è®¡ç®— SHAP å€¼...\n")

# æŠ½å–æµ‹è¯•é›†çš„å­æ ·æœ¬è¿›è¡Œ SHAP åˆ†æï¼ˆå…¨é‡è®¡ç®—å¯èƒ½è¾ƒæ…¢ï¼‰
set.seed(42)
shap_sample_size <- min(2000, nrow(X_test))
shap_idx <- sample(seq_len(nrow(X_test)), shap_sample_size)
X_shap   <- as.matrix(X_test[shap_idx, ])

# ä½¿ç”¨ shapviz ä» LightGBM æ¨¡å‹æå– SHAP å€¼
shp <- shapviz(final_model, X_pred = X_shap, X = X_shap)

cat("SHAP å€¼è®¡ç®—å®Œæˆï¼Œæ ·æœ¬æ•°:", shap_sample_size, "\n")

# 10.2 ä¿å­˜ SHAP å€¼çŸ©é˜µ
shap_values_df <- as_tibble(shp$S) %>%
  mutate(sample_id = shap_idx)
write_csv(shap_values_df, "4SHAP/shap_values.csv")
cat("âœ“ SHAP å€¼çŸ©é˜µå·²ä¿å­˜è‡³ 4SHAP/shap_values.csv\n")

# ä¿å­˜å¯¹åº”çš„ç‰¹å¾å€¼
shap_features_df <- as_tibble(shp$X) %>%
  mutate(sample_id = shap_idx)
write_csv(shap_features_df, "4SHAP/shap_feature_values.csv")
cat("âœ“ SHAP å¯¹åº”ç‰¹å¾å€¼å·²ä¿å­˜è‡³ 4SHAP/shap_feature_values.csv\n")

# 10.3 SHAP Summary Plot (èœ‚å·¢å›¾ / Beeswarm Plot)
cat("ç»˜åˆ¶ SHAP Summary Plot...\n")

p_shap_summary <- sv_importance(shp, kind = "beeswarm", show_numbers = TRUE) +
  labs(
    title    = "SHAP Summary Plot (Beeswarm)",
    subtitle = "æ¯ä¸ªç‚¹ä»£è¡¨ä¸€ä¸ªæ ·æœ¬ï¼Œé¢œè‰²è¡¨ç¤ºç‰¹å¾å€¼çš„é«˜ä½"
  ) +
  my_theme +
  theme(legend.position = "right")

ggsave("4SHAP/08_shap_summary_beeswarm.png", p_shap_summary, 
       width = 10, height = 7, dpi = 300)
cat("âœ“ SHAP Summary Beeswarm å›¾å·²ä¿å­˜\n")

# 10.4 SHAP ç‰¹å¾é‡è¦æ€§æŸ±çŠ¶å›¾ï¼ˆåŸºäºå¹³å‡ç»å¯¹ SHAP å€¼ï¼‰
p_shap_bar <- sv_importance(shp, kind = "bar", show_numbers = TRUE) +
  labs(
    title    = "SHAP ç‰¹å¾é‡è¦æ€§ (Mean |SHAP|)",
    subtitle = "åŸºäºå¹³å‡ç»å¯¹ SHAP å€¼çš„ç‰¹å¾æ’å"
  ) +
  my_theme

ggsave("4SHAP/09_shap_importance_bar.png", p_shap_bar, 
       width = 8, height = 6, dpi = 300)
cat("âœ“ SHAP ç‰¹å¾é‡è¦æ€§æŸ±çŠ¶å›¾å·²ä¿å­˜\n")

# 10.5 SHAP Dependence Plots (ä¾èµ–å›¾) â€” å¯¹ Top 4 ç‰¹å¾ç»˜åˆ¶
cat("ç»˜åˆ¶ SHAP Dependence Plots...\n")

# è·å–ç‰¹å¾é‡è¦æ€§æ’å
shap_mean_abs <- colMeans(abs(shp$S))
top_features  <- names(sort(shap_mean_abs, decreasing = TRUE))[1:4]

cat("Top 4 ç‰¹å¾:", paste(top_features, collapse = ", "), "\n")

# ä¸ºæ¯ä¸ª Top ç‰¹å¾ç»˜åˆ¶ä¾èµ–å›¾
for (feat in top_features) {
  p_dep <- sv_dependence(shp, v = feat, color_var = "auto") +
    labs(
      title    = sprintf("SHAP Dependence Plot: %s", feat),
      subtitle = "é¢œè‰²è¡¨ç¤ºäº¤äº’ç‰¹å¾çš„å–å€¼"
    ) +
    my_theme +
    theme(legend.position = "right")
  
  fname <- sprintf("4SHAP/10_shap_dependence_%s.png", feat)
  ggsave(fname, p_dep, width = 8, height = 6, dpi = 300)
  cat(sprintf("  âœ“ %s ä¾èµ–å›¾å·²ä¿å­˜\n", feat))
}

# 10.6 SHAP Force Plot â€” å•ä¸ªæ ·æœ¬è§£é‡Š
cat("ç»˜åˆ¶ SHAP Waterfall Plot (å•æ ·æœ¬è§£é‡Š)...\n")

# é€‰æ‹©ç¬¬ä¸€ä¸ªæ ·æœ¬è¿›è¡Œè§£é‡Š
p_waterfall <- sv_waterfall(shp, row_id = 1) +
  labs(
    title    = "SHAP Waterfall Plot (å•æ ·æœ¬è§£é‡Š)",
    subtitle = sprintf("æ ·æœ¬ #%d çš„ä»·æ ¼é¢„æµ‹åˆ†è§£", shap_idx[1])
  ) +
  my_theme

ggsave("4SHAP/11_shap_waterfall_sample1.png", p_waterfall, 
       width = 10, height = 7, dpi = 300)
cat("âœ“ SHAP Waterfall å›¾å·²ä¿å­˜\n")

# 10.7 SHAP Force Plot â€” å¤šä¸ªæ ·æœ¬
p_force <- sv_force(shp, row_id = 1:5) +
  labs(title = "SHAP Force Plot (å‰5ä¸ªæ ·æœ¬)") +
  my_theme

ggsave("4SHAP/12_shap_force_top5.png", p_force, 
       width = 14, height = 8, dpi = 300)
cat("âœ“ SHAP Force Plot (å¤šæ ·æœ¬) å·²ä¿å­˜\n")


# ============================================================================
# 11. ç»¼åˆå¯¹æ¯”ï¼šä¸‰ç§é‡è¦æ€§æ–¹æ³•å¯¹æ¯”
# ============================================================================

cat("\n========== 11. ç‰¹å¾é‡è¦æ€§ç»¼åˆå¯¹æ¯” ==========\n")

# 11.1 ä» CSV è¯»å–å„ç±»é‡è¦æ€§æ•°æ®
feat_imp_native <- read_csv("1.5Tables/feature_importance.csv", show_col_types = FALSE) %>%
  select(Feature, Gain) %>%
  rename(variable = Feature, value = Gain) %>%
  mutate(method = "Native (Gain)", value = value / max(value))  # å½’ä¸€åŒ–

feat_imp_perm <- read_csv("3Permutation/permutation_importance.csv", show_col_types = FALSE) %>%
  select(variable, mean_dropout_loss) %>%
  rename(value = mean_dropout_loss) %>%
  mutate(method = "Permutation", value = value / max(value))

shap_vals_csv <- read_csv("4SHAP/shap_values.csv", show_col_types = FALSE) %>%
  select(-sample_id) %>%
  summarise(across(everything(), ~ mean(abs(.)))) %>%
  pivot_longer(everything(), names_to = "variable", values_to = "value") %>%
  mutate(method = "SHAP (Mean |SHAP|)", value = value / max(value))

# åˆå¹¶
importance_comparison <- bind_rows(feat_imp_native, feat_imp_perm, shap_vals_csv)

write_csv(importance_comparison, "1.5Tables/importance_comparison.csv")

# 11.2 ç»˜åˆ¶å¯¹æ¯”å›¾
p_compare <- importance_comparison %>%
  mutate(variable = fct_reorder(variable, value, .fun = max)) %>%
  ggplot(aes(x = value, y = variable, fill = method)) +
  geom_col(position = "dodge", width = 0.7) +
  scale_fill_brewer(palette = "Set1") +
  labs(
    title    = "ç‰¹å¾é‡è¦æ€§ç»¼åˆå¯¹æ¯”",
    subtitle = "Native Gain / Permutation / SHAP ä¸‰ç§æ–¹æ³•å½’ä¸€åŒ–å¯¹æ¯”",
    x = "å½’ä¸€åŒ–é‡è¦æ€§",
    y = "ç‰¹å¾",
    fill = "æ–¹æ³•"
  ) +
  my_theme +
  theme(legend.position = "bottom")

ggsave("2Figs/14_importance_comparison.png", p_compare, 
       width = 10, height = 7, dpi = 300)
cat("âœ“ ç‰¹å¾é‡è¦æ€§ç»¼åˆå¯¹æ¯”å›¾å·²ä¿å­˜\n")

