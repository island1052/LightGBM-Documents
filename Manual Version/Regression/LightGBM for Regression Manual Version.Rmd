---
title: "LightGBM for Regression Manual Version"
output: html_document
---

# LightGBM for Regression Manual Version

This document provides a manual version of using LightGBM for regression tasks. It covers the basic steps to train a LightGBM model for regression, including data preparation, model training, and evaluation.

```         
æ•°æ®é›†ï¼šdiamondsï¼ˆé’»çŸ³ä»·æ ¼é¢„æµ‹ï¼‰ 
ç›®æ ‡å˜é‡ï¼špriceï¼ˆä»·æ ¼ï¼‰ 
ç‰¹å¾ï¼šcarat, cut, color, clarity, depth, table, x, y, z
```

## 1 Setting Up the Environment

```{r}
# 0.1 è¯»å–å½“å‰ä½ç½®ï¼Œå¹¶å°†å·¥ä½œç¯å¢ƒè®¾ç½®ä¸ºå½“å‰ä½ç½®ï¼ˆRStudio ç›¸å¯¹è·¯å¾„ï¼‰
if (requireNamespace("rstudioapi", quietly = TRUE)) {
  setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
}
cat("å½“å‰å·¥ä½œç›®å½•:", getwd(), "\n")

# 0.2 åŠ è½½åŒ…ä¸å¤šæ ¸é…ç½®
if (!require("pacman")) install.packages("pacman")
pacman::p_load(
  tidyverse,        # æ•°æ®å¤„ç†ä¸å¯è§†åŒ–
  
  lightgbm,         # LightGBM æ¨¡å‹
  recipes,          # ç‰¹å¾å·¥ç¨‹
  caret,            # ä¼ ç»Ÿå»ºæ¨¡æ¡†æ¶ï¼ˆç½‘æ ¼æœç´¢ï¼‰
  ParBayesianOptimization, # è´å¶æ–¯ä¼˜åŒ–
  shapviz,          # SHAP å¯è§†åŒ–
  DALEX,            # å¯è§£é‡Šæ€§æ¡†æ¶
  DALEXtra,         # DALEX æ‰©å±•
  parallel,         # å¹¶è¡Œè®¡ç®—
  doParallel,       # å¹¶è¡Œåç«¯
  foreach,          # å¹¶è¡Œå¾ªç¯
  tidymodels,       # ç°ä»£å»ºæ¨¡æ¡†æ¶
  Matrix,           # ç¨€ç–çŸ©é˜µ
  ggthemes,         # ggplot2 ä¸»é¢˜æ‰©å±•
  patchwork         # å›¾å½¢æ‹¼æ¥
)

# è·å–é€»è¾‘æ ¸å¿ƒæ•°ï¼Œé¢„ç•™2ä¸ªæ ¸å¿ƒç»™ç³»ç»Ÿ
n_cores <- max(1, parallel::detectCores() - 2)
cat("ä½¿ç”¨æ ¸å¿ƒæ•°:", n_cores, "\n")

# 0.3 è‡ªåŠ¨åˆ›å»ºæ–‡ä»¶å¤¹ç»“æ„
dirs <- c("0Data", "1Models", "1.5Tables", "2Figs", "3Permutation", "4SHAP")
sapply(dirs, function(d) if (!dir.exists(d)) dir.create(d, recursive = TRUE))
cat("æ–‡ä»¶å¤¹ç»“æ„å·²å°±ç»ª\n")

# è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿å¯é‡å¤æ€§
set.seed(42)
```

## 2 Data Preparation and Preprocessing

æŠŠé’»çŸ³æ•°æ®æ¸…æ´—å¹²å‡€ã€è½¬æ¢æˆé€‚åˆå»ºæ¨¡çš„æ ¼å¼ï¼Œç„¶ååˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå‡†å¤‡ç”¨LightGBMæ¥é¢„æµ‹é’»çŸ³ä»·æ ¼ã€‚

```{r}
# 1.1 åŠ è½½ diamonds æ•°æ®é›†ï¼ˆéšæœºæŠ½æ · 30%ï¼‰å› ä¸ºå¤ªå¤šäº†å®¹æ˜“æ…¢
data("diamonds", package = "ggplot2")
df_raw <- as_tibble(diamonds) %>%
  sample_frac(0.3)  # ä¿ç•™ 30% çš„æ ·æœ¬ï¼Œä½¿ç”¨æ–‡ä»¶é¡¶éƒ¨çš„ set.seed(42) ä¿è¯å¯é‡å¤æ€§

cat("å·²ä»åŸå§‹ diamonds ä¸­éšæœºæŠ½å– 50% æ ·æœ¬ã€‚\n")
cat("æŠ½æ ·åæ•°æ®ç»´åº¦:", nrow(df_raw), "è¡Œ x", ncol(df_raw), "åˆ—\n")
cat("å˜é‡å:", paste(names(df_raw), collapse = ", "), "\n")

# 1.2 æ•°æ®æ¢ç´¢
cat("\n--- æ•°æ®ç»“æ„ ---\n")
str(df_raw)

cat("\n--- æ•°æ®æ‘˜è¦ ---\n")
summary(df_raw)

# 1.3 ç¼ºå¤±å€¼æ£€æŸ¥
missing_summary <- df_raw %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(everything(), names_to = "variable", values_to = "missing_count") %>%
  mutate(missing_pct = round(missing_count / nrow(df_raw) * 100, 2))

cat("\n--- ç¼ºå¤±å€¼æ£€æŸ¥ ---\n")
print(missing_summary)

if (sum(missing_summary$missing_count) == 0) {
  cat("âœ“ æ•°æ®æ— ç¼ºå¤±å€¼\n")
} else {
  cat("âš  å­˜åœ¨ç¼ºå¤±å€¼ï¼Œå°†è¿›è¡Œå¤„ç†\n")
  df_raw <- df_raw %>% drop_na()
}

# 1.4 æ•°æ®æ¸…æ´—ï¼šç§»é™¤å¼‚å¸¸å€¼ï¼ˆx, y, z ä¸­å­˜åœ¨ 0 å€¼çš„è®°å½•ï¼‰
cat("\n--- å¼‚å¸¸å€¼æ£€æŸ¥ ---\n")
n_before <- nrow(df_raw)
df_clean <- df_raw %>%
  filter(x > 0, y > 0, z > 0)
n_after <- nrow(df_clean)
cat("ç§»é™¤ x/y/z ä¸º 0 çš„è®°å½•:", n_before - n_after, "æ¡\n")
cat("æ¸…æ´—åæ•°æ®ç»´åº¦:", nrow(df_clean), "è¡Œ x", ncol(df_clean), "åˆ—\n")

# 1.5 æœ‰åºåˆ†ç±»å˜é‡çš„æ•´æ•°ç¼–ç 
# ----------------------------------------------------------------------------
# é‡è¦è¯´æ˜ï¼š
# - LightGBM åŸç”Ÿæ”¯æŒ categorical featuresï¼Œä½†è¦æ±‚è¾“å…¥ä¸ºæ•´æ•°ç±»å‹
# - å¯¹äºæœ‰åºåˆ†ç±»å˜é‡ï¼ˆå¦‚ cut: Fair < Good < Very Good < Premium < Idealï¼‰
#   æ•´æ•°ç¼–ç å¯ä»¥ä¿ç•™é¡ºåºä¿¡æ¯ï¼Œæ¯” one-hot ç¼–ç æ›´é«˜æ•ˆ
# - æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æŒ‡å®š categorical_feature å‚æ•°ï¼Œå‘ŠçŸ¥ LightGBM å“ªäº›åˆ—æ˜¯ç±»åˆ«å‹
# - å¦‚æœå˜é‡æ˜¯æ— åºç±»åˆ«ï¼ˆå¦‚é¢œè‰²åç§°ï¼‰ï¼Œåº”è€ƒè™‘ä½¿ç”¨ one-hot æˆ– LightGBM çš„æ•´æ•°ç¼–ç +æŒ‡å®šcategory
# ----------------------------------------------------------------------------

cat("\n--- æœ‰åºåˆ†ç±»å˜é‡ç¼–ç  ---\n")

# æŸ¥çœ‹åŸå§‹å› å­æ°´å¹³
cat("cut æ°´å¹³:", levels(df_clean$cut), "\n")
cat("color æ°´å¹³:", levels(df_clean$color), "\n")
cat("clarity æ°´å¹³:", levels(df_clean$clarity), "\n")

# æ•´æ•°ç¼–ç ï¼šå°†æœ‰åºå› å­è½¬ä¸ºæ•´æ•°
df_encoded <- df_clean %>%
  mutate(
    cut     = as.integer(cut),      # Fair=1, Good=2, Very Good=3, Premium=4, Ideal=5
    color   = as.integer(color),    # D=1, E=2, ..., J=7
    clarity = as.integer(clarity)   # I1=1, SI2=2, ..., IF=8
  )

cat("ç¼–ç å®Œæˆï¼Œæ•°æ®ç±»å‹:\n")
cat(paste(names(df_encoded), "->", sapply(df_encoded, class), collapse = "\n"), "\n")

# ä¿å­˜æ¸…æ´—åçš„æ•°æ®
write_csv(df_encoded, "0Data/diamonds_cleaned.csv")
cat("âœ“ æ¸…æ´—åæ•°æ®å·²ä¿å­˜è‡³ 0Data/diamonds_cleaned.csv\n")

# data separation
# ä½¿ç”¨ tidymodels çš„ initial_split è¿›è¡Œåˆ†å±‚æŠ½æ ·ï¼ˆæŒ‰ price åˆ†å±‚ï¼‰
split_obj <- initial_split(df_encoded, prop = 0.8, strata = price)
df_train  <- training(split_obj)
df_test   <- testing(split_obj)

cat("è®­ç»ƒé›†:", nrow(df_train), "è¡Œ\n")
cat("æµ‹è¯•é›†:", nrow(df_test), "è¡Œ\n")

# åˆ†ç¦»ç‰¹å¾ä¸ç›®æ ‡å˜é‡
target_col   <- "price"
feature_cols <- setdiff(names(df_encoded), target_col)

X_train <- df_train %>% select(all_of(feature_cols))
y_train <- df_train[[target_col]]
X_test  <- df_test %>% select(all_of(feature_cols))
y_test  <- df_test[[target_col]]

# æ„å»º LightGBM æ•°æ®çŸ©é˜µ
# æŒ‡å®šåˆ†ç±»ç‰¹å¾åˆ—ï¼ˆæ•´æ•°ç¼–ç çš„æœ‰åºå˜é‡ï¼‰
cat_features <- c("cut", "color", "clarity")

dtrain <- lgb.Dataset(
  data     = as.matrix(X_train),
  label    = y_train,
  categorical_feature = cat_features
)

dtest <- lgb.Dataset(
  data      = as.matrix(X_test),
  label     = y_test,
  reference = dtrain,
  categorical_feature = cat_features
)

# ä¿å­˜è®­ç»ƒé›†å’Œæµ‹è¯•é›†
write_csv(df_train, "0Data/train_set.csv")
write_csv(df_test, "0Data/test_set.csv")
cat("âœ“ è®­ç»ƒé›†/æµ‹è¯•é›†å·²ä¿å­˜è‡³ 0Data/\n")
```

## 3 Model Training and Tuning

```{r}
# ============================================================================
# 3. æ–¹æ¡ˆ A: å¹¶è¡Œç½‘æ ¼æœç´¢
# ============================================================================
library(foreach)
library(doParallel)

# æ³¨å†Œå¹¶è¡Œåç«¯
cl <- makeCluster(n_cores)
registerDoParallel(cl)

# å‚æ•°ç½‘æ ¼
param_grid <- expand.grid(
  learning_rate  = c(0.01, 0.05, 0.1),
  num_leaves     = c(31, 63, 127),
  max_depth      = c(-1, 6, 10),
  min_data_in_leaf = c(20, 50),
  feature_fraction = c(0.8, 1.0),
  bagging_fraction = c(0.8, 1.0),
  stringsAsFactors = FALSE
)

# éšæœºæŠ½æ ·éƒ¨åˆ†å‚æ•°è¿›è¡Œæ¼”ç¤ºï¼ˆå…¨é‡è·‘å¤ªæ…¢ï¼‰
set.seed(123)
param_sample <- param_grid %>% sample_n(min(20, nrow(param_grid))) 

# --- å…³é”®ä¿®æ­£ï¼šå¯¼å‡ºåŸå§‹æ•°æ®è€Œé lgb.Dataset ---
# LightGBM Dataset æ˜¯ C++ æŒ‡é’ˆï¼Œä¸èƒ½ç›´æ¥è·¨è¿›ç¨‹ä¼ è¾“
clusterExport(cl, varlist = c("X_train", "y_train", "cat_features"), envir = environment())
clusterEvalQ(cl, {
  library(lightgbm)
  library(dplyr)
})

cat("å¼€å§‹å¹¶è¡Œç½‘æ ¼æœç´¢...\n")

grid_results <- foreach(
  i = 1:nrow(param_sample),
  .combine = bind_rows,
  .packages = c("lightgbm", "dplyr"),
  .errorhandling = "pass" # é˜²æ­¢å•ä¸ªä»»åŠ¡æŠ¥é”™å¯¼è‡´æ•´ä½“å´©æºƒ
) %dopar% {
  
  # 1. åœ¨ Worker å†…éƒ¨æ„å»º Dataset (è¿™æ˜¯å¿…é¡»çš„)
  # è™½ç„¶æœ‰é‡å¤æ„å»ºçš„å¼€é”€ï¼Œä½†ä¿è¯äº†å†…å­˜å®‰å…¨
  dtrain_worker <- lgb.Dataset(
    data = as.matrix(X_train),
    label = y_train,
    categorical_feature = cat_features,
    free_raw_data = FALSE,
    params = list(verbose = -1)
  )
  
  # 2. è®¾å®šå‚æ•°
  params_i <- list(
    objective        = "regression",
    metric           = "rmse",
    learning_rate    = param_sample$learning_rate[i],
    num_leaves       = param_sample$num_leaves[i],
    max_depth        = param_sample$max_depth[i],
    min_data_in_leaf = param_sample$min_data_in_leaf[i],
    feature_fraction = param_sample$feature_fraction[i],
    bagging_fraction = param_sample$bagging_fraction[i],
    bagging_freq     = 5,
    verbose          = -1,
    num_threads      = 1 # å…³é”®ï¼šWorkerå†…éƒ¨å¼ºåˆ¶å•çº¿ç¨‹ï¼Œé˜²æ­¢CPUè¿‡è½½
  )
  
  # 3. äº¤å‰éªŒè¯
  cv_result <- tryCatch({
    lgb.cv(
      params   = params_i,
      data     = dtrain_worker,
      nrounds  = 1000, 
      nfold    = 3,     # æ¼”ç¤ºç”¨3æŠ˜ï¼Œå®é™…å»ºè®®5æŠ˜
      early_stopping_rounds = 30,
      verbose  = -1,
      stratified = FALSE # å›å½’ä»»åŠ¡é€šå¸¸è®¾ä¸º FALSE
    )
  }, error = function(e) return(NULL))
  
  if(is.null(cv_result)) return(NULL)
  
  # 4. è¿”å›ç»“æœ
  tibble(
    learning_rate    = params_i$learning_rate,
    num_leaves       = params_i$num_leaves,
    max_depth        = params_i$max_depth,
    best_iter        = cv_result$best_iter,
    best_rmse        = cv_result$best_score
  )
}

stopCluster(cl)
registerDoSEQ()

# å¤„ç†ç»“æœ
grid_results_df <- grid_results %>% arrange(best_rmse)
cat("ç½‘æ ¼æœç´¢æœ€ä¼˜ RMSE:", min(grid_results_df$best_rmse), "\n")
# ============================================================================
# 3.2 æ–¹æ³•Bï¼šè´å¶æ–¯ä¼˜åŒ–
# ============================================================================
# è´å¶æ–¯ä¼˜åŒ– vs ç½‘æ ¼æœç´¢ï¼š
# 
# ç½‘æ ¼æœç´¢çš„å±€é™æ€§ï¼š
#   - ç»´æ•°ç¾éš¾ï¼šå‚æ•°æ¯å¢åŠ ä¸€ä¸ªç»´åº¦ï¼Œæœç´¢æ¬¡æ•°æŒ‡æ•°å¢é•¿
#   - ç›²ç›®æœç´¢ï¼šæ¯æ¬¡è¯•éªŒç‹¬ç«‹ï¼Œä¸åˆ©ç”¨å†å²ä¿¡æ¯
#
# è´å¶æ–¯ä¼˜åŒ–çš„ä¼˜åŠ¿ï¼š
#   - åˆ©ç”¨é«˜æ–¯è¿‡ç¨‹ä»£ç†æ¨¡å‹ï¼Œæ ¹æ®å†å²ç»“æœæ™ºèƒ½é€‰æ‹©ä¸‹ä¸€ç»„å‚æ•°
#   - å¹³è¡¡æ¢ç´¢ï¼ˆexplorationï¼‰å’Œåˆ©ç”¨ï¼ˆexploitationï¼‰
#   - é€šå¸¸èƒ½åœ¨æ›´å°‘çš„è¿­ä»£æ¬¡æ•°å†…æ‰¾åˆ°æ›´ä¼˜è§£
#
# å‚æ•°è¾¹ç•Œè®¾ç½®è¯´æ˜ï¼š
#   learning_rate: è¿‡å¤§å¯èƒ½å¯¼è‡´ä¸æ”¶æ•›ï¼Œè¿‡å°è®­ç»ƒå¤ªæ…¢
#   num_leaves: æ§åˆ¶æ¨¡å‹å¤æ‚åº¦ï¼Œè¿‡å¤§æ˜“è¿‡æ‹Ÿåˆ
#   min_data_in_leaf: é˜²æ­¢å¶å­èŠ‚ç‚¹æ ·æœ¬å¤ªå°‘å¯¼è‡´çš„è¿‡æ‹Ÿåˆ
#   lambda_l1/l2: æ­£åˆ™åŒ–å‚æ•°ï¼Œåº”å¯¹å¤šé‡å…±çº¿æ€§
# ----------------------------------------------------------------------------

lgb_bayesian_objective <- function(
    learning_rate, num_leaves, max_depth, min_data_in_leaf,
    feature_fraction, bagging_fraction, lambda_l1, lambda_l2
) {
  
  # 1. åŠ è½½åŒ…
  library(lightgbm)
  
  # 2. orkerèŠ‚ç‚¹é‡æ–°åˆ›å»ºæ•°æ®ï¼
  # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦è®¿é—®X_train, y_train, cat_features
  # è¿™äº›å¯¹è±¡å¿…é¡»ä»ä¸»èŠ‚ç‚¹å¯¼å‡º
  
  dtrain_worker <- lgb.Dataset(
    data = as.matrix(X_train),
    label = y_train,
    categorical_feature = cat_features
  )
  
  # 3. è®¾ç½®å‚æ•°
  params <- list(
    objective        = "regression",
    metric           = "rmse",
    learning_rate    = learning_rate,
    num_leaves       = as.integer(num_leaves),
    max_depth        = as.integer(max_depth),
    min_data_in_leaf = as.integer(min_data_in_leaf),
    feature_fraction = feature_fraction,
    bagging_fraction = bagging_fraction,
    lambda_l1        = lambda_l1,
    lambda_l2        = lambda_l2,
    bagging_freq     = 5,
    verbose          = -1,
    num_threads      = 1
  )
  
  # 4. æ‰§è¡ŒCV
  cv_result <- lgb.cv(
    params   = params,
    data     = dtrain_worker,  # ä½¿ç”¨workerè‡ªå·±çš„æ•°æ®
    nrounds  = 1000,
    nfold    = 5,
    early_stopping_rounds = 50,
    verbose  = -1
  )
  
  list(Score = -cv_result$best_score, Pred = 0)
}

# ============================================================================
# å¹¶è¡Œé…ç½®
# ============================================================================

library(parallel)
library(doParallel)
library(lightgbm)

# è®¾ç½®å¹¶è¡Œé›†ç¾¤
n_cores <- max(1, detectCores() - 2)
cl <- makeCluster(n_cores)

# æ­¥éª¤1ï¼šåœ¨workerèŠ‚ç‚¹åŠ è½½åŒ…
clusterEvalQ(cl, {
  library(lightgbm)
  NULL
})
search_bounds <- list(
  learning_rate = c(0.01, 0.3),
  num_leaves = c(20, 100),
  max_depth = c(3, 10),
  min_data_in_leaf = c(5, 50),      # æ·»åŠ è¿™ä¸ªå‚æ•°ï¼
  feature_fraction = c(0.5, 1.0),    # æ·»åŠ è¿™ä¸ªå‚æ•°ï¼
  bagging_fraction = c(0.5, 1.0),    # æ·»åŠ è¿™ä¸ªå‚æ•°ï¼
  lambda_l1 = c(0, 1),              # æ·»åŠ è¿™ä¸ªå‚æ•°ï¼
  lambda_l2 = c(0, 1)               # æ·»åŠ è¿™ä¸ªå‚æ•°ï¼
)
# 2ï¼šæŠŠæ•°æ®å¯¼å‡ºåˆ°workerèŠ‚ç‚¹
clusterExport(cl, 
              c("X_train", "y_train", "cat_features" , "search_bounds"),  # æ³¨æ„ï¼šæ˜¯dtrainçš„åŸææ–™ï¼Œä¸æ˜¯dtrainæœ¬èº«ï¼
              envir = environment())

registerDoParallel(cl)

# æ‰§è¡Œè´å¶æ–¯ä¼˜åŒ–
set.seed(42)
bayes_result <- bayesOpt(
  FUN       = lgb_bayesian_objective,
  bounds    = search_bounds,
  initPoints = 10,
  iters.n   = 20,
  iters.k   = 5,
  parallel  = TRUE,
  acq       = "ucb",
  kappa     = 2.576,
  verbose   = 1
)

# æ¸…ç†
stopCluster(cl)
registerDoSEQ()
cat("\n--- è´å¶æ–¯ä¼˜åŒ–æœ€ä¼˜å‚æ•° ---\n")
best_bayes_params <- getBestPars(bayes_result)
print(best_bayes_params)

# ä¿å­˜è´å¶æ–¯ä¼˜åŒ–å†å²
bayes_history <- bayes_result$scoreSummary %>%
  as_tibble()
write_csv(bayes_history, "1.5Tables/bayesian_optimization_history.csv")
cat("âœ“ è´å¶æ–¯ä¼˜åŒ–å†å²å·²ä¿å­˜è‡³ 1.5Tables/bayesian_optimization_history.csv\n")

# ä½¿ç”¨æœ€ä¼˜å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
# 6.1 å† å†›æ¨¡å‹é€‰æ‹©ç­–ç•¥
# ----------------------------------------------------------------------------
# ä¸ºä»€ä¹ˆéœ€è¦ä¸¤ç§ä¼˜åŒ–æ–¹æ³•ï¼Ÿ
# 1. ç½‘æ ¼æœç´¢ï¼šå…¨é¢ä½†è®¡ç®—é‡å¤§ï¼Œé€‚åˆå°è§„æ¨¡å‚æ•°ç©ºé—´
# 2. è´å¶æ–¯ä¼˜åŒ–ï¼šæ™ºèƒ½ä½†å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜
#
# æœ¬æ–‡ç¨¿é‡‡ç”¨"åŒé‡ä¿é™©"ç­–ç•¥ï¼š
#   - å…ˆç”¨ç½‘æ ¼æœç´¢å¿«é€Ÿæ‰«æå¹¿æ³›å‚æ•°ç©ºé—´
#   - å†ç”¨è´å¶æ–¯ä¼˜åŒ–åœ¨æœ€ä¼˜åŒºåŸŸç²¾ç»†æœç´¢
#   - æœ€ç»ˆé€‰æ‹©éªŒè¯é›† RMSE æ›´ä½çš„æ¨¡å‹
#
# å®é™…åº”ç”¨ä¸­ï¼Œå¦‚æœè®¡ç®—èµ„æºå……è¶³ï¼Œå¯ä»¥ï¼š
#   - å…ˆç”¨ç½‘æ ¼æœç´¢ç²—ç­›ï¼Œç¡®å®šå‚æ•°èŒƒå›´
#   - å†ç”¨è´å¶æ–¯ä¼˜åŒ–åœ¨ç¼©å°åçš„ç©ºé—´ç²¾ç»†è°ƒä¼˜
# ----------------------------------------------------------------------------
cat("ä»ä¸¤ç§ä¼˜åŒ–æ–¹æ³•ä¸­ç­›é€‰å† å†›æ¨¡å‹...\n")

# ç½‘æ ¼æœç´¢çš„æœ€ä½³å‚æ•°
best_grid_params <- grid_results %>% slice(1)
grid_rmse <- best_grid_params$best_rmse

# è´å¶æ–¯ä¼˜åŒ–çš„æœ€ä½³å‚æ•°
bayes_rmse <- bayes_result$scoreSummary %>%
  as_tibble() %>%
  filter(Score == max(Score)) %>%
  slice(1) %>%
  pull(Score) %>%
  abs()  # è´å¶æ–¯ä¼˜åŒ–å­˜å‚¨çš„æ˜¯è´Ÿå€¼

cat("\nç½‘æ ¼æœç´¢æœ€ä½³ RMSE:", grid_rmse, "\n")
cat("è´å¶æ–¯ä¼˜åŒ–æœ€ä½³ RMSE:", bayes_rmse, "\n")

# é€‰æ‹©ä¸¤è€…ä¸­æ›´ä¼˜çš„ï¼ˆRMSE æ›´å°ï¼‰
if (grid_rmse <= bayes_rmse) {
  cat("\nğŸ† å† å†›æ¨¡å‹: ç½‘æ ¼æœç´¢\n")
  final_params <- list(
    objective        = "regression",
    metric           = "rmse",
    learning_rate    = best_grid_params$learning_rate,
    num_leaves       = as.integer(best_grid_params$num_leaves),
    max_depth        = as.integer(best_grid_params$max_depth),
    min_data_in_leaf = as.integer(best_grid_params$min_data_in_leaf),
    feature_fraction = best_grid_params$feature_fraction,
    bagging_fraction = best_grid_params$bagging_fraction,
    bagging_freq     = 5,
    verbose          = -1
  )
  champion_source <- "Grid Search"
} else {
  cat("\nğŸ† å† å†›æ¨¡å‹: è´å¶æ–¯ä¼˜åŒ–\n")
  final_params <- list(
    objective        = "regression",
    metric           = "rmse",
    learning_rate    = best_bayes_params$learning_rate,
    num_leaves       = as.integer(best_bayes_params$num_leaves),
    max_depth        = as.integer(best_bayes_params$max_depth),
    min_data_in_leaf = as.integer(best_bayes_params$min_data_in_leaf),
    feature_fraction = best_bayes_params$feature_fraction,
    bagging_fraction = best_bayes_params$bagging_fraction,
    bagging_freq     = 5,
    lambda_l1        = best_bayes_params$lambda_l1,
    lambda_l2        = best_bayes_params$lambda_l2,
    verbose          = -1
  )
  champion_source <- "Bayesian Optimization"
}

cat("\nå† å†›æ¨¡å‹å‚æ•°:\n")
print(final_params)


cat("æœ€ç»ˆå‚æ•°:\n")
print(final_params)

# 6.1 äº¤å‰éªŒè¯ç¡®å®šæœ€ä¼˜è¿­ä»£è½®æ•°
cat("\nç¡®å®šæœ€ä¼˜è¿­ä»£è½®æ•°...\n")
cv_final <- lgb.cv(
  params   = final_params,
  data     = dtrain,
  nrounds  = 2000,
  nfold    = 5,
  early_stopping_rounds = 100,
  verbose  = -1
)

best_nrounds <- cv_final$best_iter
cat("æœ€ä¼˜è¿­ä»£è½®æ•°:", best_nrounds, "\n")
cat("CV æœ€ä¼˜ RMSE:", cv_final$best_score, "\n")

# 6.2 ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®è®­ç»ƒæœ€ç»ˆæ¨¡å‹
final_model <- lgb.train(
  params  = final_params,
  data    = dtrain,
  nrounds = best_nrounds,
  verbose = -1
)

# 6.3 ä¿å­˜æ¨¡å‹
lgb.save(final_model, "1Models/lightgbm_final_model.txt")
cat("âœ“ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³ 1Models/lightgbm_final_model.txt\n")

```

## 4 Model Evaluation and Figure data preparation

```{r}
# åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹
y_pred <- predict(final_model, as.matrix(X_test))
# 4.1 è¯„ä¼°æŒ‡æ ‡çš„é€‰æ‹©
# ----------------------------------------------------------------------------
# ä¸ºä»€ä¹ˆé€‰æ‹©è¿™å››ç§æŒ‡æ ‡ï¼Ÿ
#
# RMSE (å‡æ–¹æ ¹è¯¯å·®)ï¼š
#   - å¯¹å¤§è¯¯å·®æ›´æ•æ„Ÿï¼Œé€‚åˆéœ€è¦æƒ©ç½šä¸¥é‡åç¦»çš„åœºæ™¯
#   - å•ä½ä¸åŸå§‹å˜é‡ä¸€è‡´ï¼Œä¾¿äºè§£é‡Š
#
# MAE (å¹³å‡ç»å¯¹è¯¯å·®)ï¼š
#   - å¯¹å¼‚å¸¸å€¼æ›´ç¨³å¥
#   - åæ˜ é¢„æµ‹è¯¯å·®çš„å¹³å‡æ°´å¹³
#
# RÂ² (å†³å®šç³»æ•°)ï¼š
#   - è¡¡é‡æ¨¡å‹è§£é‡Šçš„æ–¹å·®æ¯”ä¾‹
#   - æ— é‡çº²ï¼Œä¾¿äºä¸åŒæ¨¡å‹é—´æ¯”è¾ƒ
#
# MAPE (å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®)ï¼š
#   - ç›¸å¯¹è¯¯å·®æŒ‡æ ‡ï¼Œé€‚åˆä¸åŒé‡çº²çš„æ¯”è¾ƒ
#   - æ³¨æ„ï¼šå½“çœŸå®å€¼æ¥è¿‘0æ—¶ä¼šå¤±çœŸ
# ----------------------------------------------------------------------------
# è®¡ç®—è¯„ä¼°æŒ‡æ ‡
calc_metrics <- function(actual, predicted) {
  rmse_val <- sqrt(mean((actual - predicted)^2))
  mae_val  <- mean(abs(actual - predicted))
  ss_res   <- sum((actual - predicted)^2)
  ss_tot   <- sum((actual - mean(actual))^2)
  r2_val   <- 1 - ss_res / ss_tot
  mape_val <- mean(abs((actual - predicted) / actual)) * 100
  
  tibble(
    Metric = c("RMSE", "MAE", "R_squared", "MAPE"),
    Value  = c(rmse_val, mae_val, r2_val, mape_val)
  )
}

metrics_df <- calc_metrics(y_test, y_pred)
cat("\n--- æ¨¡å‹è¯„ä¼°æŒ‡æ ‡ ---\n")
print(metrics_df)

# ä¿å­˜é¢„æµ‹ç»“æœ
prediction_df <- tibble(
  actual    = y_test,
  predicted = y_pred,
  residual  = y_test - y_pred
) %>%
  bind_cols(X_test)  # é™„åŠ ç‰¹å¾ä¿¡æ¯

write_csv(prediction_df, "1.5Tables/prediction_results.csv")
cat("âœ“ é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³ 1.5Tables/prediction_results.csv\n")

# ä¿å­˜è¯„ä¼°æŒ‡æ ‡
write_csv(metrics_df, "1.5Tables/model_metrics.csv")
cat("âœ“ è¯„ä¼°æŒ‡æ ‡å·²ä¿å­˜è‡³ 1.5Tables/model_metrics.csv\n")

# ä¿å­˜ç‰¹å¾é‡è¦æ€§
importance_df <- lgb.importance(final_model) %>%
  as_tibble()
write_csv(importance_df, "1.5Tables/feature_importance.csv")
cat("âœ“ ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜è‡³ 1.5Tables/feature_importance.csv\n")

cat("\nç‰¹å¾é‡è¦æ€§æ’å:\n")
print(importance_df)

```

## 5 Figure Drawing and Interpretation

```{r}
my_theme <- theme_few(base_size = 12) +
  theme(
    plot.title    = element_text(face = "bold", hjust = 0.5, size = 14),
    plot.subtitle = element_text(hjust = 0.5, color = "grey40"),
    axis.title    = element_text(face = "bold"),
    legend.position = "bottom"
  )

# ---- 5.1 ä» CSV è¯»å–æ•°æ® ----
cat("ä» CSV æ–‡ä»¶è¯»å–æ•°æ®è¿›è¡Œç»˜å›¾...\n")

importance_plot_df  <- read_csv("1.5Tables/feature_importance.csv", show_col_types = FALSE)
prediction_plot_df  <- read_csv("1.5Tables/prediction_results.csv", show_col_types = FALSE)
metrics_plot_df     <- read_csv("1.5Tables/model_metrics.csv", show_col_types = FALSE)

# ---- 5.2 ç‰¹å¾é‡è¦æ€§å›¾ ----
cat("ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾...\n")

p_importance <- importance_plot_df %>%
  mutate(Feature = fct_reorder(Feature, Gain)) %>%
  ggplot(aes(x = Gain, y = Feature, fill = Gain)) +
  geom_col(show.legend = FALSE, width = 0.7) +
  scale_fill_gradient(low = "#6BAED6", high = "#08519C") +
  labs(
    title    = "LightGBM ç‰¹å¾é‡è¦æ€§ (Gain)",
    subtitle = "åŸºäºä¿¡æ¯å¢ç›Šçš„ç‰¹å¾æ’å",
    x = "ä¿¡æ¯å¢ç›Š (Gain)",
    y = "ç‰¹å¾"
  ) +
  my_theme

ggsave("2Figs/01_feature_importance.png", p_importance, 
       width = 8, height = 6, dpi = 300)
cat("âœ“ ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜è‡³ 2Figs/01_feature_importance.png\n")

# é¢å¤–ï¼šå¤šç»´åº¦ç‰¹å¾é‡è¦æ€§å›¾ï¼ˆGain + Cover + Frequencyï¼‰
p_importance_multi <- importance_plot_df %>%
  pivot_longer(cols = c(Gain, Cover, Frequency), 
               names_to = "Metric", values_to = "Value") %>%
  mutate(Feature = fct_reorder(Feature, Value, .fun = max)) %>%
  ggplot(aes(x = Value, y = Feature, fill = Metric)) +
  geom_col(position = "dodge", width = 0.7) +
  scale_fill_brewer(palette = "Set2") +
  facet_wrap(~Metric, scales = "free_x") +
  labs(
    title    = "LightGBM å¤šç»´åº¦ç‰¹å¾é‡è¦æ€§",
    subtitle = "Gain / Cover / Frequency ä¸‰ç»´åº¦å¯¹æ¯”",
    x = "é‡è¦æ€§æ•°å€¼",
    y = "ç‰¹å¾"
  ) +
  my_theme

ggsave("2Figs/02_feature_importance_multi.png", p_importance_multi, 
       width = 12, height = 6, dpi = 300)
cat("âœ“ å¤šç»´åº¦ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜è‡³ 2Figs/02_feature_importance_multi.png\n")

# ---- 5.3 é¢„æµ‹å€¼ vs çœŸå®å€¼æ•£ç‚¹å›¾ ----
cat("ç»˜åˆ¶é¢„æµ‹å€¼ vs çœŸå®å€¼æ•£ç‚¹å›¾...\n")

# è¯»å–è¯„ä¼°æŒ‡æ ‡ç”¨äºæ ‡æ³¨
rmse_val <- metrics_plot_df %>% filter(Metric == "RMSE") %>% pull(Value)
r2_val   <- metrics_plot_df %>% filter(Metric == "R_squared") %>% pull(Value)
mae_val  <- metrics_plot_df %>% filter(Metric == "MAE") %>% pull(Value)

annotation_text <- sprintf("RMSE = %.2f\nMAE = %.2f\nRÂ² = %.4f", rmse_val, mae_val, r2_val)

p_actual_vs_pred <- prediction_plot_df %>%
  ggplot(aes(x = actual, y = predicted)) +
  geom_point(alpha = 0.15, color = "#2171B5", size = 0.8) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed", linewidth = 1) +
  annotate("text", x = min(prediction_plot_df$actual) + 500, 
           y = max(prediction_plot_df$predicted) - 1000,
           label = annotation_text, hjust = 0, size = 4, fontface = "bold",
           color = "darkred") +
  labs(
    title    = "é¢„æµ‹å€¼ vs çœŸå®å€¼",
    subtitle = "çº¢è‰²è™šçº¿ä¸ºå®Œç¾é¢„æµ‹å‚è€ƒçº¿",
    x = "çœŸå®ä»·æ ¼ (Actual Price)",
    y = "é¢„æµ‹ä»·æ ¼ (Predicted Price)"
  ) +
  coord_equal() +
  my_theme

ggsave("2Figs/03_actual_vs_predicted.png", p_actual_vs_pred, 
       width = 8, height = 8, dpi = 300)
cat("âœ“ é¢„æµ‹å€¼ vs çœŸå®å€¼æ•£ç‚¹å›¾å·²ä¿å­˜è‡³ 2Figs/03_actual_vs_predicted.png\n")
# 5.4 æ®‹å·®è¯Šæ–­çš„ç›®çš„
# ----------------------------------------------------------------------------
# æ®‹å·®åˆ†ææ˜¯æ£€éªŒæ¨¡å‹å‡è®¾çš„é‡è¦æ‰‹æ®µï¼š
#
# 1. æ®‹å·®ç›´æ–¹å›¾ + å¯†åº¦æ›²çº¿ï¼š
#    - æ£€æŸ¥æ˜¯å¦å‘ˆæ­£æ€åˆ†å¸ƒï¼ˆå›å½’æ¨¡å‹çš„åŸºæœ¬å‡è®¾ï¼‰
#    - åæ€åˆ†å¸ƒå¯èƒ½æç¤ºéœ€è¦å˜æ¢ç›®æ ‡å˜é‡
#
# 2. æ®‹å·® vs é¢„æµ‹å€¼æ•£ç‚¹å›¾ï¼š
#    - æ£€æŸ¥å¼‚æ–¹å·®æ€§ï¼ˆæ®‹å·®æ˜¯å¦éšé¢„æµ‹å€¼å˜åŒ–ï¼‰
#    - çº¢è‰²å¹³æ»‘æ›²çº¿åº”æ¥è¿‘æ°´å¹³çº¿ï¼Œå¦åˆ™æç¤ºæ¨¡å‹æœ‰ç³»ç»Ÿæ€§åå·®
#    - å–‡å­å½¢åˆ†å¸ƒæç¤ºéœ€è¦åŠ æƒå›å½’æˆ–å¯¹æ•°å˜æ¢
#
# 3. QQ å›¾ï¼š
#    - ç‚¹åº”å¤§è‡´è½åœ¨45åº¦çº¿ä¸Š
#    - åç¦»æç¤ºéæ­£æ€æ€§ï¼Œå¯èƒ½å½±å“ç½®ä¿¡åŒºé—´çš„å‡†ç¡®æ€§
# ----------------------------------------------------------------------------
# ---- 5.4 æ®‹å·®åˆ†å¸ƒå›¾ ----
cat("ç»˜åˆ¶æ®‹å·®åˆ†å¸ƒå›¾...\n")

# æ®‹å·®ç›´æ–¹å›¾
p_residual_hist <- prediction_plot_df %>%
  ggplot(aes(x = residual)) +
  geom_histogram(aes(y = after_stat(density)), bins = 60, 
                 fill = "#6BAED6", color = "white", alpha = 0.8) +
  geom_density(color = "#08519C", linewidth = 1) +
  geom_vline(xintercept = 0, color = "red", linetype = "dashed", linewidth = 0.8) +
  labs(
    title    = "æ®‹å·®åˆ†å¸ƒç›´æ–¹å›¾",
    subtitle = sprintf("æ®‹å·®å‡å€¼ = %.2f, æ ‡å‡†å·® = %.2f", 
                       mean(prediction_plot_df$residual), 
                       sd(prediction_plot_df$residual)),
    x = "æ®‹å·® (Actual - Predicted)",
    y = "å¯†åº¦"
  ) +
  my_theme

# æ®‹å·® vs é¢„æµ‹å€¼æ•£ç‚¹å›¾
p_residual_scatter <- prediction_plot_df %>%
  ggplot(aes(x = predicted, y = residual)) +
  geom_point(alpha = 0.15, color = "#2171B5", size = 0.8) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed", linewidth = 0.8) +
  geom_smooth(method = "loess", se = TRUE, color = "#E6550D", linewidth = 1) +
  labs(
    title    = "æ®‹å·® vs é¢„æµ‹å€¼",
    subtitle = "æ£€æŸ¥å¼‚æ–¹å·®æ€§å’Œç³»ç»Ÿæ€§åå·®",
    x = "é¢„æµ‹ä»·æ ¼ (Predicted Price)",
    y = "æ®‹å·® (Residual)"
  ) +
  my_theme

# åˆå¹¶æ®‹å·®å›¾
p_residual_combined <- p_residual_hist + p_residual_scatter +
  plot_annotation(
    title    = "æ®‹å·®è¯Šæ–­",
    theme    = theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5))
  )

ggsave("2Figs/04_residual_analysis.png", p_residual_combined, 
       width = 14, height = 6, dpi = 300)
cat("âœ“ æ®‹å·®åˆ†æå›¾å·²ä¿å­˜è‡³ 2Figs/04_residual_analysis.png\n")

# ---- 5.5 QQ å›¾ ----
p_qq <- prediction_plot_df %>%
  ggplot(aes(sample = residual)) +
  stat_qq(alpha = 0.3, color = "#2171B5") +
  stat_qq_line(color = "red", linewidth = 1) +
  labs(
    title = "æ®‹å·® Q-Q å›¾",
    subtitle = "æ£€éªŒæ®‹å·®æ­£æ€æ€§",
    x = "ç†è®ºåˆ†ä½æ•°",
    y = "æ ·æœ¬åˆ†ä½æ•°"
  ) +
  my_theme

ggsave("2Figs/05_residual_qq_plot.png", p_qq, 
       width = 7, height = 7, dpi = 300)
cat("âœ“ QQ å›¾å·²ä¿å­˜è‡³ 2Figs/05_residual_qq_plot.png\n")

```

## 6 Permutation Importance and SHAP Values

```{r}
# ============================================================================
# 6. Permutation Importance (ç½®æ¢é‡è¦æ€§åˆ†æ)
# ============================================================================
# åŸç†è§£è¯»ï¼š
# 
# ç½®æ¢é‡è¦æ€§æ˜¯ä¸€ç§æ¨¡å‹æ— å…³çš„ç‰¹å¾è¯„ä¼°æ–¹æ³•ï¼š
# 1. é¦–å…ˆè®¡ç®—æ¨¡å‹åœ¨åŸå§‹æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ï¼ˆRMSEï¼‰
# 2. å¯¹ç‰¹å¾åˆ—éšæœºæ‰“ä¹±ï¼ˆç ´åç‰¹å¾ä¸ç›®æ ‡çš„å…³ç³»ï¼‰
# 3. é‡æ–°é¢„æµ‹å¹¶è®¡ç®—æ€§èƒ½ä¸‹é™ç¨‹åº¦
# 4. é‡å¤å¤šæ¬¡ä»¥è·å¾—ç¨³å®šä¼°è®¡
#
# ä¼˜åŠ¿ï¼š
#   - ç›´è§‚æ˜“æ‡‚ï¼šç›´æ¥è¡¡é‡ç‰¹å¾ç ´ååå¯¹æ€§èƒ½çš„å½±å“
#   - æ¨¡å‹æ— å…³ï¼šå¯ç”¨äºä»»ä½•æ¨¡å‹
#   - è€ƒè™‘äº†ç‰¹å¾é—´çš„äº¤äº’ä½œç”¨
#
# æ³¨æ„äº‹é¡¹ï¼š
#   - ç›¸å…³ç‰¹å¾å¯èƒ½å¯¼è‡´é‡è¦æ€§ä½ä¼°ï¼ˆæ‰“ä¹±ä¸€ä¸ªï¼Œå¦ä¸€ä¸ªè¿˜èƒ½æä¾›ä¿¡æ¯ï¼‰
#   - B = 500 æ¬¡ç½®æ¢æä¾›ç¨³å®šçš„ç»Ÿè®¡æ¨æ–­
#   - p-value è®¡ç®—ï¼šå®é™…è§‚å¯Ÿå€¼åœ¨ç½®æ¢åˆ†å¸ƒä¸­çš„ä½ç½®
# ----------------------------------------------------------------------------

cat("\n========== 6. Permutation Importance ==========\n")
# 6.1 ä½¿ç”¨ DALEX åˆ›å»º explainer
# è‡ªå®šä¹‰é¢„æµ‹å‡½æ•°
predict_fun <- function(model, newdata) {
  predict(model, as.matrix(newdata))
}

explainer <- explain(
  model          = final_model,
  data           = as.data.frame(X_test),
  y              = y_test,
  predict_function = predict_fun,
  label          = "LightGBM",
  verbose        = FALSE
)

# 6.2 è®¡ç®—ç½®æ¢é‡è¦æ€§
cat("è®¡ç®— Permutation Importance...\n")
perm_importance <- model_parts(
  explainer,
  loss_function = loss_root_mean_square,
  B             = 500,     # ç½®æ¢æ¬¡æ•°
  type          = "difference"
)

# 6.3 ä¿å­˜ç½®æ¢é‡è¦æ€§ç»“æœ
perm_df <- perm_importance %>%
  as_tibble() %>%
  filter(variable != "_full_model_" & variable != "_baseline_") %>%
  group_by(variable) %>%
  summarise(
    mean_dropout_loss = mean(dropout_loss, na.rm = TRUE),
    sd_dropout_loss   = sd(dropout_loss, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  arrange(desc(mean_dropout_loss))

write_csv(perm_df, "3Permutation/permutation_importance.csv")
cat("âœ“ ç½®æ¢é‡è¦æ€§ç»“æœå·²ä¿å­˜è‡³ 3Permutation/permutation_importance.csv\n")
cat("\nç½®æ¢é‡è¦æ€§æ’å:\n")
print(perm_df)

# 6.4 ä¿å­˜å®Œæ•´çš„ç½®æ¢é‡è¦æ€§ï¼ˆæ¯æ¬¡ç½®æ¢çš„ç»“æœï¼‰
perm_full_df <- perm_importance %>%
  as_tibble()
write_csv(perm_full_df, "3Permutation/permutation_importance_full.csv")

# 6.5 ç»˜åˆ¶ç½®æ¢é‡è¦æ€§å›¾ï¼ˆä» CSV è¯»å–ï¼‰
cat("ç»˜åˆ¶ Permutation Importance å›¾...\n")

perm_plot_df <- read_csv("3Permutation/permutation_importance.csv", show_col_types = FALSE)

p_perm <- perm_plot_df %>%
  mutate(variable = fct_reorder(variable, mean_dropout_loss)) %>%
  ggplot(aes(x = mean_dropout_loss, y = variable)) +
  geom_col(fill = "#FB6A4A", width = 0.7) +
  geom_errorbarh(
    aes(xmin = mean_dropout_loss - sd_dropout_loss,
        xmax = mean_dropout_loss + sd_dropout_loss),
    height = 0.3, color = "grey30"
  ) +
  labs(
    title    = "Permutation Importance",
    subtitle = "ç‰¹å¾è¢«ç½®æ¢å RMSE çš„å¢åŠ é‡ï¼ˆè¶Šå¤§è¶Šé‡è¦ï¼‰",
    x = "RMSE å¢åŠ é‡ (Dropout Loss Difference)",
    y = "ç‰¹å¾"
  ) +
  my_theme

ggsave("3Permutation/06_permutation_importance.png", p_perm, 
       width = 8, height = 6, dpi = 300)
cat("âœ“ Permutation Importance å›¾å·²ä¿å­˜è‡³ 3Permutation/06_permutation_importance.png\n")

# 6.6 ä½¿ç”¨ DALEX å†…ç½®ç»˜å›¾ï¼ˆé¢å¤–å‚è€ƒï¼‰
p_perm_dalex <- plot(perm_importance) +
  labs(title = "Permutation Importance (DALEX)") +
  my_theme

ggsave("3Permutation/07_permutation_importance_dalex.png", p_perm_dalex, 
       width = 8, height = 6, dpi = 300)
cat("âœ“ DALEX Permutation å›¾å·²ä¿å­˜\n")
# ============================================================================
# 6.7 ç½®æ¢é‡è¦æ€§åˆ†å¸ƒç›´æ–¹å›¾
# ============================================================================

cat("ç»˜åˆ¶ç½®æ¢é‡è¦æ€§åˆ†å¸ƒç›´æ–¹å›¾...\n")

# ä»å®Œæ•´ç½®æ¢ç»“æœä¸­è®¡ç®—åˆ†å¸ƒ
perm_full_df <- read_csv("3Permutation/permutation_importance_full.csv", 
                          show_col_types = FALSE)

# è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
perm_stats <- perm_full_df %>%
  filter(variable != "_full_model_" & variable != "_baseline_") %>%
  group_by(variable) %>%
  summarise(
    mean_loss = mean(dropout_loss, na.rm = TRUE),
    sd_loss   = sd(dropout_loss, na.rm = TRUE),
    min_loss  = min(dropout_loss, na.rm = TRUE),
    max_loss  = max(dropout_loss, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  arrange(desc(mean_loss))

# é€‰æ‹© Top 3 ç‰¹å¾åˆ†åˆ«ç»˜åˆ¶åˆ†å¸ƒ
top_3_features <- perm_stats %>% slice(1:3) %>% pull(variable)

for (feat in top_3_features) {
  
  # æå–è¯¥ç‰¹å¾çš„æ‰€æœ‰ç½®æ¢ç»“æœ
  feat_perms <- perm_full_df %>%
    filter(variable == feat & variable != "_full_model_" & variable != "_baseline_") %>%
    pull(dropout_loss)
  
  # è®¡ç®—ç»Ÿè®¡é‡
  mean_perm <- mean(feat_perms, na.rm = TRUE)
  sd_perm   <- sd(feat_perms, na.rm = TRUE)
  
  # è®¡ç®— p-valueï¼ˆå®é™…è§‚æµ‹å€¼ç›¸å¯¹äºç½®æ¢åˆ†å¸ƒçš„æ’åï¼‰
  actual_loss <- perm_df %>% filter(variable == feat) %>% pull(mean_dropout_loss)
  p_val <- mean(feat_perms <= actual_loss, na.rm = TRUE)
  
  # ç»˜åˆ¶ç›´æ–¹å›¾
  p_perm_dist <- tibble(dropout_loss = feat_perms) %>%
    ggplot(aes(x = dropout_loss)) +
    geom_histogram(aes(y = after_stat(density)), 
                   bins = 50, fill = "#BDBDBD", color = "white", alpha = 0.8) +
    geom_density(color = "black", linewidth = 0.8) +
    # çº¢çº¿ï¼šçœŸå®è§‚å¯Ÿå€¼
    geom_vline(xintercept = actual_loss, color = "red", 
               linetype = "solid", linewidth = 1.5) +
    # æ³¨é‡Š
    annotate("text", x = Inf, y = Inf, 
             label = sprintf("Real Accuracy: %.3f\np-value: %.4f", actual_loss, p_val),
             hjust = 1.05, vjust = 1.1, 
             fontface = "bold", color = "darkred", size = 4) +
    labs(
      title    = sprintf("Permutation Importance Distribution: %s", feat),
      subtitle = "ç›´æ–¹å›¾ = 500 æ¬¡ç½®æ¢çš„åˆ†å¸ƒï¼Œçº¢çº¿ = å®é™…è§‚æµ‹å€¼",
      x = "Dropout Loss (RMSE increase)",
      y = "Density"
    ) +
    my_theme +
    theme(plot.subtitle = element_text(size = 11, color = "grey30"))
  
  # ä¿å­˜å›¾ç‰‡
  fname <- sprintf("3Permutation/08_perm_distribution_%s.png", feat)
  ggsave(fname, p_perm_dist, width = 8, height = 6, dpi = 300)
  cat(sprintf("  âœ“ %s ç½®æ¢åˆ†å¸ƒå›¾å·²ä¿å­˜\n", feat))
}

# ç»˜åˆ¶æ‰€æœ‰ç‰¹å¾çš„åˆ†å¸ƒå¯¹æ¯”ï¼ˆå°æç´å›¾ï¼‰
p_perm_violin <- perm_full_df %>%
  filter(variable != "_full_model_" & variable != "_baseline_") %>%
  mutate(variable = fct_reorder(variable, dropout_loss, .fun = median)) %>%
  ggplot(aes(x = variable, y = dropout_loss, fill = variable)) +
  geom_violin(alpha = 0.6, show.legend = FALSE) +
  geom_boxplot(width = 0.1, color = "black", fill = "white") +
  geom_point(data = perm_df, 
             aes(x = variable, y = mean_dropout_loss),
             color = "red", size = 3, shape = 4) +
  labs(
    title    = "Permutation Importance Distribution (All Features)",
    subtitle = "çº¢å‰ = å¹³å‡ç½®æ¢é‡è¦æ€§ï¼Œå°æç´ = åˆ†å¸ƒ",
    x = "ç‰¹å¾",
    y = "Dropout Loss"
  ) +
  coord_flip() +
  my_theme

ggsave("3Permutation/09_perm_distributions_violin.png", p_perm_violin, 
       width = 8, height = 6, dpi = 300)
cat("âœ“ ç½®æ¢é‡è¦æ€§å°æç´å›¾å·²ä¿å­˜\n")
# ============================================================================
# 7. SHAP åˆ†æ (SHapley Additive exPlanations)
# ============================================================================
# SHAP å€¼åŸºäºåšå¼ˆè®ºä¸­çš„ Shapley å€¼ï¼š
#
# æ ¸å¿ƒæ€æƒ³ï¼šæ¯ä¸ªç‰¹å¾å€¼çš„"è´¡çŒ®"æ˜¯å…¶å¯¹é¢„æµ‹å€¼çš„å¹³å‡è¾¹é™…è´¡çŒ®
#
# å‡ ç§å›¾è¡¨çš„è§£è¯»ï¼š
#
# 1. Beeswarm Summary Plotï¼š
#    - Xè½´ï¼šSHAPå€¼ï¼ˆå¯¹é¢„æµ‹çš„å½±å“æ–¹å‘å’Œå¤§ï¼‰
#    - Yè½´ï¼šç‰¹å¾æŒ‰é‡è¦æ€§æ’åº
#    - é¢œè‰²ï¼šç‰¹å¾å€¼é«˜ä½ï¼ˆçº¢è‰²é«˜å€¼ï¼Œè“è‰²ä½å€¼ï¼‰
#    - è§£è¯»ç¤ºä¾‹ï¼šcarat å€¼è¶Šé«˜ï¼ˆè¶Šçº¢ï¼‰ï¼ŒSHAPå€¼è¶Šå¤§ï¼ˆä»·æ ¼è¶Šé«˜ï¼‰
#
# 2. Dependence Plotï¼š
#    - æ˜¾ç¤ºç‰¹å¾å€¼ä¸SHAPå€¼çš„å…³ç³»
#    - é¢œè‰²è¡¨ç¤ºäº¤äº’ç‰¹å¾ï¼Œå¸®åŠ©å‘ç°éçº¿æ€§å…³ç³»å’Œäº¤äº’ä½œç”¨
#
# 3. Waterfall Plotï¼š
#    - å•ä¸ªæ ·æœ¬çš„é¢„æµ‹åˆ†è§£
#    - ä»åŸºçº¿å€¼ï¼ˆå¹³å‡é¢„æµ‹ï¼‰å¼€å§‹ï¼Œæ¯ä¸ªç‰¹å¾å¢åŠ /å‡å°‘è´¡çŒ®
#    - ç›´è§‚å±•ç¤ºæ¨¡å‹å¦‚ä½•åšå‡ºç‰¹å®šé¢„æµ‹
# ----------------------------------------------------------------------------

cat("\n========== 7. SHAP åˆ†æ ==========\n")

# 7.1 è®¡ç®— SHAP å€¼
# ä½¿ç”¨ shapviz åŒ…ï¼Œç›´æ¥è°ƒç”¨ LightGBM çš„ SHAP è®¡ç®—
cat("è®¡ç®— SHAP å€¼...\n")

# æŠ½å–æµ‹è¯•é›†çš„å­æ ·æœ¬è¿›è¡Œ SHAP åˆ†æï¼ˆå…¨é‡è®¡ç®—å¯èƒ½è¾ƒæ…¢ï¼‰
set.seed(42)
shap_sample_size <- min(2000, nrow(X_test))
shap_idx <- sample(seq_len(nrow(X_test)), shap_sample_size)
X_shap   <- as.matrix(X_test[shap_idx, ])

# ä½¿ç”¨ shapviz ä» LightGBM æ¨¡å‹æå– SHAP å€¼
shp <- shapviz(final_model, X_pred = X_shap, X = X_shap)

cat("SHAP å€¼è®¡ç®—å®Œæˆï¼Œæ ·æœ¬æ•°:", shap_sample_size, "\n")

# 7.2 ä¿å­˜ SHAP å€¼çŸ©é˜µ
shap_values_df <- as_tibble(shp$S) %>%
  mutate(sample_id = shap_idx)
write_csv(shap_values_df, "4SHAP/shap_values.csv")
cat("âœ“ SHAP å€¼çŸ©é˜µå·²ä¿å­˜è‡³ 4SHAP/shap_values.csv\n")

# ä¿å­˜å¯¹åº”çš„ç‰¹å¾å€¼
shap_features_df <- as_tibble(shp$X) %>%
  mutate(sample_id = shap_idx)
write_csv(shap_features_df, "4SHAP/shap_feature_values.csv")
cat("âœ“ SHAP å¯¹åº”ç‰¹å¾å€¼å·²ä¿å­˜è‡³ 4SHAP/shap_feature_values.csv\n")

# 7.3 SHAP Summary Plot (èœ‚å·¢å›¾ / Beeswarm Plot)
cat("ç»˜åˆ¶ SHAP Summary Plot...\n")

p_shap_summary <- sv_importance(shp, kind = "beeswarm", show_numbers = TRUE) +
  labs(
    title    = "SHAP Summary Plot (Beeswarm)",
    subtitle = "æ¯ä¸ªç‚¹ä»£è¡¨ä¸€ä¸ªæ ·æœ¬ï¼Œé¢œè‰²è¡¨ç¤ºç‰¹å¾å€¼çš„é«˜ä½"
  ) +
  my_theme +
  theme(legend.position = "right")

ggsave("4SHAP/08_shap_summary_beeswarm.png", p_shap_summary, 
       width = 10, height = 7, dpi = 300)
cat("âœ“ SHAP Summary Beeswarm å›¾å·²ä¿å­˜\n")

# 7.4 SHAP ç‰¹å¾é‡è¦æ€§æŸ±çŠ¶å›¾ï¼ˆåŸºäºå¹³å‡ç»å¯¹ SHAP å€¼ï¼‰
p_shap_bar <- sv_importance(shp, kind = "bar", show_numbers = TRUE) +
  labs(
    title    = "SHAP ç‰¹å¾é‡è¦æ€§ (Mean |SHAP|)",
    subtitle = "åŸºäºå¹³å‡ç»å¯¹ SHAP å€¼çš„ç‰¹å¾æ’å"
  ) +
  my_theme

ggsave("4SHAP/09_shap_importance_bar.png", p_shap_bar, 
       width = 8, height = 6, dpi = 300)
cat("âœ“ SHAP ç‰¹å¾é‡è¦æ€§æŸ±çŠ¶å›¾å·²ä¿å­˜\n")

# 7.5 SHAP Dependence Plots (ä¾èµ–å›¾) â€” å¯¹ Top 4 ç‰¹å¾ç»˜åˆ¶
cat("ç»˜åˆ¶ SHAP Dependence Plots...\n")

# è·å–ç‰¹å¾é‡è¦æ€§æ’å
shap_mean_abs <- colMeans(abs(shp$S))
top_features  <- names(sort(shap_mean_abs, decreasing = TRUE))[1:4]

cat("Top 4 ç‰¹å¾:", paste(top_features, collapse = ", "), "\n")

# ä¸ºæ¯ä¸ª Top ç‰¹å¾ç»˜åˆ¶ä¾èµ–å›¾
for (feat in top_features) {
  p_dep <- sv_dependence(shp, v = feat, color_var = "auto") +
    labs(
      title    = sprintf("SHAP Dependence Plot: %s", feat),
      subtitle = "é¢œè‰²è¡¨ç¤ºäº¤äº’ç‰¹å¾çš„å–å€¼"
    ) +
    my_theme +
    theme(legend.position = "right")
  
  fname <- sprintf("4SHAP/10_shap_dependence_%s.png", feat)
  ggsave(fname, p_dep, width = 8, height = 6, dpi = 300)
  cat(sprintf("  âœ“ %s ä¾èµ–å›¾å·²ä¿å­˜\n", feat))
}

# 7.6 SHAP Force Plot â€” å•ä¸ªæ ·æœ¬è§£é‡Š
cat("ç»˜åˆ¶ SHAP Waterfall Plot (å•æ ·æœ¬è§£é‡Š)...\n")

# é€‰æ‹©ç¬¬ä¸€ä¸ªæ ·æœ¬è¿›è¡Œè§£é‡Š
p_waterfall <- sv_waterfall(shp, row_id = 1) +
  labs(
    title    = "SHAP Waterfall Plot (å•æ ·æœ¬è§£é‡Š)",
    subtitle = sprintf("æ ·æœ¬ #%d çš„ä»·æ ¼é¢„æµ‹åˆ†è§£", shap_idx[1])
  ) +
  my_theme

ggsave("4SHAP/11_shap_waterfall_sample1.png", p_waterfall, 
       width = 10, height = 7, dpi = 300)
cat("âœ“ SHAP Waterfall å›¾å·²ä¿å­˜\n")

# 7.7 SHAP Force Plot â€” å¤šä¸ªæ ·æœ¬
p_force <- sv_force(shp, row_id = 1:5) +
  labs(title = "SHAP Force Plot (å‰5ä¸ªæ ·æœ¬)") +
  my_theme

ggsave("4SHAP/12_shap_force_top5.png", p_force, 
       width = 14, height = 8, dpi = 300)
cat("âœ“ SHAP Force Plot (å¤šæ ·æœ¬) å·²ä¿å­˜\n")


# ============================================================================
# ç»¼åˆå¯¹æ¯”ï¼šä¸‰ç§é‡è¦æ€§æ–¹æ³•å¯¹æ¯”
# ============================================================================

cat("\n========== 11. ç‰¹å¾é‡è¦æ€§ç»¼åˆå¯¹æ¯” ==========\n")

# 11.1 ä» CSV è¯»å–å„ç±»é‡è¦æ€§æ•°æ®
feat_imp_native <- read_csv("1.5Tables/feature_importance.csv", show_col_types = FALSE) %>%
  select(Feature, Gain) %>%
  rename(variable = Feature, value = Gain) %>%
  mutate(method = "Native (Gain)", value = value / max(value))  # å½’ä¸€åŒ–

feat_imp_perm <- read_csv("3Permutation/permutation_importance.csv", show_col_types = FALSE) %>%
  select(variable, mean_dropout_loss) %>%
  rename(value = mean_dropout_loss) %>%
  mutate(method = "Permutation", value = value / max(value))

shap_vals_csv <- read_csv("4SHAP/shap_values.csv", show_col_types = FALSE) %>%
  select(-sample_id) %>%
  summarise(across(everything(), ~ mean(abs(.)))) %>%
  pivot_longer(everything(), names_to = "variable", values_to = "value") %>%
  mutate(method = "SHAP (Mean |SHAP|)", value = value / max(value))

# åˆå¹¶
importance_comparison <- bind_rows(feat_imp_native, feat_imp_perm, shap_vals_csv)

write_csv(importance_comparison, "1.5Tables/importance_comparison.csv")

# 11.2 ç»˜åˆ¶å¯¹æ¯”å›¾
p_compare <- importance_comparison %>%
  mutate(variable = fct_reorder(variable, value, .fun = max)) %>%
  ggplot(aes(x = value, y = variable, fill = method)) +
  geom_col(position = "dodge", width = 0.7) +
  scale_fill_brewer(palette = "Set1") +
  labs(
    title    = "ç‰¹å¾é‡è¦æ€§ç»¼åˆå¯¹æ¯”",
    subtitle = "Native Gain / Permutation / SHAP ä¸‰ç§æ–¹æ³•å½’ä¸€åŒ–å¯¹æ¯”",
    x = "å½’ä¸€åŒ–é‡è¦æ€§",
    y = "ç‰¹å¾",
    fill = "æ–¹æ³•"
  ) +
  my_theme +
  theme(legend.position = "bottom")

ggsave("2Figs/14_importance_comparison.png", p_compare, 
       width = 10, height = 7, dpi = 300)
cat("âœ“ ç‰¹å¾é‡è¦æ€§ç»¼åˆå¯¹æ¯”å›¾å·²ä¿å­˜\n")
```
